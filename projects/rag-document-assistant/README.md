# üß™ RAG Science Assistant ‚Äî Syst√®me de Recherche Documentaire Intelligent

> [!IMPORTANT]
> **üîí Confidentialit√© & Donn√©es de D√©monstration :**  
> Pour des raisons de confidentialit√©, les documents acad√©miques originaux de l'**ENSAI** ne sont pas expos√©s dans ce d√©p√¥t public.  
> Pour la d√©monstration, le syst√®me a √©t√© aliment√© avec un corpus d'**articles scientifiques publics** portant sur la **d√©tection d'anomalies**, permettant ainsi de tester toutes les fonctionnalit√©s de recherche et de citation sans compromettre de donn√©es sensibles.

## üéØ Aper√ßu du Projet
Ce projet est un assistant de recherche capable d'extraire des informations pertinentes depuis un corpus de documents scientifiques volumineux. Il utilise la technique du **RAG (Retrieval-Augmented Generation)** pour fournir des r√©ponses pr√©cises, sourc√©es et sans hallucinations.

### ‚ùì Pourquoi ce projet ?
L'IA g√©n√©rative classique (LLM) peut "halluciner" si elle n'a pas acc√®s √† un contexte sp√©cifique. Ce syst√®me garantit :
- **Pr√©cision** : Les r√©ponses sont bas√©es uniquement sur des documents r√©els.
- **Transparence** : Chaque affirmation est accompagn√©e d'une citation directe de la source (Page, Extrait).
- **Flexibilit√©** : Fonctionne avec n'importe quel ensemble de PDFs.

## üõ†Ô∏è Architecture Technique
Le syst√®me repose sur une stack moderne et performante :
- **Extraction & Parsing** : Traitement des PDFs via LangChain (`PyPDFLoader`).
- **Indexation Vectorielle** : Utilisation de **FAISS** (Facebook AI Similarity Search) pour la recherche s√©mantique ultra-rapide.
- **Embeddings** : Mod√®le `all-MiniLM-L6-v2` de HuggingFace (pour une utilisation locale efficace).
- **Cerveau (LLM)** : **Mistral-7B** via **Ollama** pour une ex√©cution 100% locale et priv√©e.
- **Interface & API** : **FastAPI** pour une API robuste et document√©e (Swagger UI).

## üöÄ Installation & Lancement Local

### 1. Pr√©requis
- **Python 3.11+**
- **Ollama** install√© sur votre machine ([ollama.com](https://ollama.com))

### 2. Configuration d'Ollama
T√©l√©chargez le mod√®le Mistral :
```bash
ollama pull mistral
```

### 3. Installation des d√©pendances
```bash
pip install -r requirements.txt
```

### 4. Lancement de l'API
Ouvrez un terminal √† la racine du projet et lancez :
```bash
uvicorn app.api:app --reload
```
L'interface Swagger sera alors accessible sur : `http://localhost:8000/docs`.

## üìÇ Structure du Projet
- `app/` : Code source de l'API et du pipeline RAG.
- `data/` : Dossier contenant les documents PDFs √† indexer.
- `faiss_index/` : Stockage local de l'index vectoriel.
- `requirements.txt` : Liste des d√©pendances Python.

## ü§ù Contribution
Ce projet a √©t√© d√©velopp√© dans le cadre d'un portfolio pour d√©montrer des comp√©tences en **NLP**, **Vector Databases** et **Architecture LLM**.
