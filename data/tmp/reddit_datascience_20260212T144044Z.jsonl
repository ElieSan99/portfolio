{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1r2flqg", "created_utc": 1770858446.0, "title": "Meta ds - interview", "selftext": "I just read on blind that meta is squeezing its ds team and plans to automate it completely in a year. Can anyone, working with meta confirm if true? I have an upcoming interview for product analytics position and I am wondering if I should take it if it is a hire for fire positon?", "full_text": "Meta ds - interview\n\nI just read on blind that meta is squeezing its ds team and plans to automate it completely in a year. Can anyone, working with meta confirm if true? I have an upcoming interview for product analytics position and I am wondering if I should take it if it is a hire for fire positon?", "author": "No-Mud4063", "permalink": "/r/datascience/comments/1r2flqg/meta_ds_interview/", "url": "https://www.reddit.com/r/datascience/comments/1r2flqg/meta_ds_interview/", "score": 25, "num_comments": 14, "upvote_ratio": 0.76, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1r2argg", "created_utc": 1770846563.0, "title": "Anyone ever work with Meta data?", "selftext": "specifically looking to try and attribute ads to conversions. the clean room solutions that Meta supports seems really expensive. anyone ever spend enough money to get exposure logs or maybe even use a CRM to help attribution?", "full_text": "Anyone ever work with Meta data?\n\nspecifically looking to try and attribute ads to conversions. the clean room solutions that Meta supports seems really expensive. anyone ever spend enough money to get exposure logs or maybe even use a CRM to help attribution?", "author": "gengarvibes", "permalink": "/r/datascience/comments/1r2argg/anyone_ever_work_with_meta_data/", "url": "https://www.reddit.com/r/datascience/comments/1r2argg/anyone_ever_work_with_meta_data/", "score": 8, "num_comments": 6, "upvote_ratio": 0.78, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1r24okt", "created_utc": 1770833118.0, "title": "Rescaling logistic regression predictions for under-sampled data?", "selftext": "I'm building a predictive model for a large dataset with a binary 0/1 outcome that is heavily imbalanced.\n\nI'm under-sampling records from the majority outcome class (the 0s) in order to fit the data into my computer's memory prior to fitting a logistic regression model.\n\nBecause of the under-sampling, do I need to rescale the model's probability predictions when choosing the optimal threshold or is the scale arbitrary?", "full_text": "Rescaling logistic regression predictions for under-sampled data?\n\nI'm building a predictive model for a large dataset with a binary 0/1 outcome that is heavily imbalanced.\n\nI'm under-sampling records from the majority outcome class (the 0s) in order to fit the data into my computer's memory prior to fitting a logistic regression model.\n\nBecause of the under-sampling, do I need to rescale the model's probability predictions when choosing the optimal threshold or is the scale arbitrary?", "author": "RobertWF_47", "permalink": "/r/datascience/comments/1r24okt/rescaling_logistic_regression_predictions_for/", "url": "https://www.reddit.com/r/datascience/comments/1r24okt/rescaling_logistic_regression_predictions_for/", "score": 10, "num_comments": 14, "upvote_ratio": 0.82, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1r21ce9", "created_utc": 1770825900.0, "title": "New Study Finds AI May Be Leading to “Workload Creep” in Tech", "selftext": "", "full_text": "New Study Finds AI May Be Leading to “Workload Creep” in Tech", "author": "warmeggnog", "permalink": "/r/datascience/comments/1r21ce9/new_study_finds_ai_may_be_leading_to_workload/", "url": "https://www.interviewquery.com/p/ai-workload-creep-tech-workers-study", "score": 268, "num_comments": 30, "upvote_ratio": 0.97, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1r1qo10", "created_utc": 1770793973.0, "title": "[Advice/Vent] How to coach an insular and combative science team", "selftext": "My startup was acquired by a legacy enterprise. We were primarily acquired for our technical talent and some high growth ML products they see as a strategic threat. \n\nTheir ML team is entirely entry-level and struggling badly. They have very poor fundamentals around labeling training data, build systems without strong business cases, and ignore reasonable feedback from engineering partners regarding latency and safe deployment patterns. \n\nI am staff level MLE and have been asked to up level this team. I’ve tried the following:\n\n\\- Being inquisitive and asking them to explain design decisions \n\n\\- walking them through our systems and discussing the good/bad/ugly\n\n\\- being vulnerable about past decisions that were suboptimal\n\n\\- offering to provide feedback before design review with cross functional partners\n\nNone of this has worked. I am mostly ignored. When I point out something obvious (e.g 12 second latency is unacceptable for live inference) they claim there is no time to fix it. They write dozens of pages of documents that do not have answers to simple questions (what ML algorithms are you using? What data do you need at inference time? What systems rely on your responses).  They then claim no one is knowledgeable enough to understand their approach. It seems like when something doesn’t go their way they just stonewall and gaslight.\n\nI personally have never dealt with this before. I’m curious if anyone has coached a team to unlearn these behaviors and heal cross functional relationships.\n\nMy advice right now is to break apart the team and either help them find non-ML roles internally or let them go. ", "full_text": "[Advice/Vent] How to coach an insular and combative science team\n\nMy startup was acquired by a legacy enterprise. We were primarily acquired for our technical talent and some high growth ML products they see as a strategic threat. \n\nTheir ML team is entirely entry-level and struggling badly. They have very poor fundamentals around labeling training data, build systems without strong business cases, and ignore reasonable feedback from engineering partners regarding latency and safe deployment patterns. \n\nI am staff level MLE and have been asked to up level this team. I’ve tried the following:\n\n\\- Being inquisitive and asking them to explain design decisions \n\n\\- walking them through our systems and discussing the good/bad/ugly\n\n\\- being vulnerable about past decisions that were suboptimal\n\n\\- offering to provide feedback before design review with cross functional partners\n\nNone of this has worked. I am mostly ignored. When I point out something obvious (e.g 12 second latency is unacceptable for live inference) they claim there is no time to fix it. They write dozens of pages of documents that do not have answers to simple questions (what ML algorithms are you using? What data do you need at inference time? What systems rely on your responses).  They then claim no one is knowledgeable enough to understand their approach. It seems like when something doesn’t go their way they just stonewall and gaslight.\n\nI personally have never dealt with this before. I’m curious if anyone has coached a team to unlearn these behaviors and heal cross functional relationships.\n\nMy advice right now is to break apart the team and either help them find non-ML roles internally or let them go.", "author": "TalkIcy2357", "permalink": "/r/datascience/comments/1r1qo10/advicevent_how_to_coach_an_insular_and_combative/", "url": "https://www.reddit.com/r/datascience/comments/1r1qo10/advicevent_how_to_coach_an_insular_and_combative/", "score": 62, "num_comments": 26, "upvote_ratio": 0.92, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1r1966a", "created_utc": 1770748916.0, "title": "2026 State of Data Engineering Survey", "selftext": "Site includes the survey data in addition to the results so you can drill in.", "full_text": "2026 State of Data Engineering Survey\n\nSite includes the survey data in addition to the results so you can drill in.", "author": "Bazencourt", "permalink": "/r/datascience/comments/1r1966a/2026_state_of_data_engineering_survey/", "url": "https://joereis.github.io/practical_data_data_eng_survey/", "score": 6, "num_comments": 0, "upvote_ratio": 0.88, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1r16y9s", "created_utc": 1770744186.0, "title": "AI isn’t making data science interviews easier.", "selftext": "I sit in hiring loops for data science/analytics roles, and I see a lot of discussion lately about AI “making interviews obsolete” or “making prep pointless.” From the interviewer side, that’s not what’s happening.\n\nThere’s a lot of posts about how you can easily generate a SQL query or even a full analysis plan using AI, but it only means we make interviews harder and more intentional, i.e. focusing more on how you think rather than whether you can come up with the correct/perfect answers.\n\nSome concrete shifts I’ve seen mainly include SQL interviews getting a lot of follow-ups, like assumptions about the data or how you’d explain query limitations to a PM/the rest of the team.\n\nFor modeling questions, the focus is more on judgment. So don’t just practice answering which model you’d use, but also think about how to communicate constraints, failure modes, trade-offs, etc.\n\nEssentially, don’t just rely on AI to generate answers. You still have to do the explaining and thinking yourself, and that requires deeper practice.\n\nI’m curious though how data science/analytics candidates are experiencing this. Has anything changed with your interview experience in light of AI? Have you adapted your interview prep to accommodate this shift (if any)?", "full_text": "AI isn’t making data science interviews easier.\n\nI sit in hiring loops for data science/analytics roles, and I see a lot of discussion lately about AI “making interviews obsolete” or “making prep pointless.” From the interviewer side, that’s not what’s happening.\n\nThere’s a lot of posts about how you can easily generate a SQL query or even a full analysis plan using AI, but it only means we make interviews harder and more intentional, i.e. focusing more on how you think rather than whether you can come up with the correct/perfect answers.\n\nSome concrete shifts I’ve seen mainly include SQL interviews getting a lot of follow-ups, like assumptions about the data or how you’d explain query limitations to a PM/the rest of the team.\n\nFor modeling questions, the focus is more on judgment. So don’t just practice answering which model you’d use, but also think about how to communicate constraints, failure modes, trade-offs, etc.\n\nEssentially, don’t just rely on AI to generate answers. You still have to do the explaining and thinking yourself, and that requires deeper practice.\n\nI’m curious though how data science/analytics candidates are experiencing this. Has anything changed with your interview experience in light of AI? Have you adapted your interview prep to accommodate this shift (if any)?", "author": "KitchenTaste7229", "permalink": "/r/datascience/comments/1r16y9s/ai_isnt_making_data_science_interviews_easier/", "url": "https://www.reddit.com/r/datascience/comments/1r16y9s/ai_isnt_making_data_science_interviews_easier/", "score": 179, "num_comments": 55, "upvote_ratio": 0.96, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1r160kr", "created_utc": 1770742180.0, "title": "[AMA] We’re dbt Labs, ask us anything!", "selftext": "", "full_text": "[AMA] We’re dbt Labs, ask us anything!", "author": "andersdellosnubes", "permalink": "/r/datascience/comments/1r160kr/ama_were_dbt_labs_ask_us_anything/", "url": "/r/dataengineering/comments/1r0ff3b/ama_were_dbt_labs_ask_us_anything/", "score": 1, "num_comments": 0, "upvote_ratio": 0.57, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1r0dvmw", "created_utc": 1770665333.0, "title": "An easy process to make sure your executive team understands the data", "selftext": "A lot of teams struggle making reports digestible for executive teams. When we report data with all the complexity of the methods, limitations, confounds, and measurements of uncertainty, management tends to respond with a common refrain:\n\n**\"Keep it simple. The executives can't wrap their minds around all of this.\"**\n\nBut there's a simple, two-step method you can use to make sure your data reports are always understood by the people in charge:\n\n1. Fire the executives\n2. Celebrate getting rid of the dead weight\n\nYou'll find this makes every part of your work faster, better, and more enjoyable.", "full_text": "An easy process to make sure your executive team understands the data\n\nA lot of teams struggle making reports digestible for executive teams. When we report data with all the complexity of the methods, limitations, confounds, and measurements of uncertainty, management tends to respond with a common refrain:\n\n**\"Keep it simple. The executives can't wrap their minds around all of this.\"**\n\nBut there's a simple, two-step method you can use to make sure your data reports are always understood by the people in charge:\n\n1. Fire the executives\n2. Celebrate getting rid of the dead weight\n\nYou'll find this makes every part of your work faster, better, and more enjoyable.", "author": "takenorinvalid", "permalink": "/r/datascience/comments/1r0dvmw/an_easy_process_to_make_sure_your_executive_team/", "url": "https://www.reddit.com/r/datascience/comments/1r0dvmw/an_easy_process_to_make_sure_your_executive_team/", "score": 336, "num_comments": 28, "upvote_ratio": 0.97, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1r0d6fi", "created_utc": 1770663858.0, "title": "You can select points with a lasso now using matplotlib", "selftext": "If you want to give it a spin, there's a marimo notebook demo right here:\n\n[https://koaning.github.io/wigglystuff/examples/chartselect/](https://koaning.github.io/wigglystuff/examples/chartselect/)", "full_text": "You can select points with a lasso now using matplotlib\n\nIf you want to give it a spin, there's a marimo notebook demo right here:\n\n[https://koaning.github.io/wigglystuff/examples/chartselect/](https://koaning.github.io/wigglystuff/examples/chartselect/)", "author": "cantdutchthis", "permalink": "/r/datascience/comments/1r0d6fi/you_can_select_points_with_a_lasso_now_using/", "url": "https://youtu.be/c-nasIVbAoM", "score": 22, "num_comments": 0, "upvote_ratio": 0.89, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1r09ynb", "created_utc": 1770657029.0, "title": "Memory exhaustion errors (crosspost from snowflake forum)", "selftext": "", "full_text": "Memory exhaustion errors (crosspost from snowflake forum)", "author": "RobertWF_47", "permalink": "/r/datascience/comments/1r09ynb/memory_exhaustion_errors_crosspost_from_snowflake/", "url": "/r/snowflake/comments/1r07w5u/memory_exhaustion_errors/", "score": 1, "num_comments": 3, "upvote_ratio": 0.67, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qzv8yw", "created_utc": 1770613297.0, "title": "Weekly Entering &amp; Transitioning - Thread 09 Feb, 2026 - 16 Feb, 2026", "selftext": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "full_text": "Weekly Entering &amp; Transitioning - Thread 09 Feb, 2026 - 16 Feb, 2026\n\n \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "author": "AutoModerator", "permalink": "/r/datascience/comments/1qzv8yw/weekly_entering_transitioning_thread_09_feb_2026/", "url": "https://www.reddit.com/r/datascience/comments/1qzv8yw/weekly_entering_transitioning_thread_09_feb_2026/", "score": 8, "num_comments": 5, "upvote_ratio": 0.84, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qytqug", "created_utc": 1770509187.0, "title": "Thoughts about going from Senior data scientist at company A to Senior Data Analyst at Company B", "selftext": "The senior data analyst at company B is significant higher pay ($50k/year more) and scope seems to be bigger with more ownership \n\nWhat kind of setback (if any) does losing the data scientist title have? ", "full_text": "Thoughts about going from Senior data scientist at company A to Senior Data Analyst at Company B\n\nThe senior data analyst at company B is significant higher pay ($50k/year more) and scope seems to be bigger with more ownership \n\nWhat kind of setback (if any) does losing the data scientist title have?", "author": "StatGoddess", "permalink": "/r/datascience/comments/1qytqug/thoughts_about_going_from_senior_data_scientist/", "url": "https://www.reddit.com/r/datascience/comments/1qytqug/thoughts_about_going_from_senior_data_scientist/", "score": 85, "num_comments": 48, "upvote_ratio": 0.91, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qyhmtx", "created_utc": 1770480382.0, "title": "Retraining strategy with evolving classes + imbalanced labels?", "selftext": "Hi all — I’m looking for advice on the best retraining strategy for a multi-class classifier in a setting where the label space can evolve. Right now I have about 6 labels, but I don’t know how many will show up over time, and some labels appear inconsistently or disappear for long stretches. My initial labeled dataset is \\~6,000 rows and it’s extremely imbalanced: one class dominates and the smallest class has only a single example. New data keeps coming in, and my boss wants us to retrain using the model’s inferences plus the human corrections made afterward by someone with domain knowledge. I have concerns about retraining on inferences, but that's a different story.\n\nGiven this setup, should retraining typically use all accumulated labeled data, a sliding window of recent data, or something like a recent window plus a replay buffer for rare but important classes? Would incremental/online learning (e.g., partial\\_fit style updates or stream-learning libraries) help here, or is periodic full retraining generally safer with this kind of label churn and imbalance? I’d really appreciate any recommendations on a robust policy that won’t collapse into the dominant class, plus how you’d evaluate it (e.g., fixed “golden” test set vs rolling test, per-class metrics) when new labels can appear.", "full_text": "Retraining strategy with evolving classes + imbalanced labels?\n\nHi all — I’m looking for advice on the best retraining strategy for a multi-class classifier in a setting where the label space can evolve. Right now I have about 6 labels, but I don’t know how many will show up over time, and some labels appear inconsistently or disappear for long stretches. My initial labeled dataset is \\~6,000 rows and it’s extremely imbalanced: one class dominates and the smallest class has only a single example. New data keeps coming in, and my boss wants us to retrain using the model’s inferences plus the human corrections made afterward by someone with domain knowledge. I have concerns about retraining on inferences, but that's a different story.\n\nGiven this setup, should retraining typically use all accumulated labeled data, a sliding window of recent data, or something like a recent window plus a replay buffer for rare but important classes? Would incremental/online learning (e.g., partial\\_fit style updates or stream-learning libraries) help here, or is periodic full retraining generally safer with this kind of label churn and imbalance? I’d really appreciate any recommendations on a robust policy that won’t collapse into the dominant class, plus how you’d evaluate it (e.g., fixed “golden” test set vs rolling test, per-class metrics) when new labels can appear.", "author": "fleeced-artichoke", "permalink": "/r/datascience/comments/1qyhmtx/retraining_strategy_with_evolving_classes/", "url": "https://www.reddit.com/r/datascience/comments/1qyhmtx/retraining_strategy_with_evolving_classes/", "score": 20, "num_comments": 7, "upvote_ratio": 0.96, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qy7a89", "created_utc": 1770448438.0, "title": "How I scraped 5.3 million jobs (including 5,335 data science jobs)", "selftext": "**Background**\n\nDuring my PhD in Data Science at Stanford, I got sick and tired of ghost jobs &amp; 3rd party offshore agencies on LinkedIn &amp; Indeed. So I wrote a script that fetches jobs from 30k+ company websites' career pages and uses GPT4o-mini to extract relevant information (ex salary, remote, etc.) from job descriptions. You can use it here: ([HiringCafe](http://hiring.cafe)). Here is a filter for Data science jobs (5,335 and counting). I scrape every company 3x/day, so the results stay fresh if you check back the next day.\n\nYou can follow my progress on r/hiringcafe\n\n**How I built the HiringCafe (from a DS perspective)**\n\n1. I identified company career pages with active job listings. I used the [Apollo.io](http://apollo.io/) to search for companies across various industries, and get their company URLs. To narrow these down, I wrote a web crawler (using Node.js, and a combination of Cheerio + Puppeteer depending on site complexity) to find the career page of the company. I discovered that I could dump the raw HTML and prompt ChatGPT o1-mini to classify (as a binary classification) whether each page contained a job description or not. I thus compiled a list of verified job page if it contains a job description or not. If it contains a job description, I add it to a list and proceed to step 2\n2. Verifying legit companies. This part I had to do manually, but it was crucial that I exclude any recruiting firms, 3rd party offshore agencies, etc. because I wanted only high-quality companies directly hiring for roles at their firm. I manually sorted through the 30,000 company career pages (this took several weeks) and picked the ones that looked legit. At Stanford, we call this technique \"occular regression\" :) It was doable because I only had to verify each company a single time and then I trust it moving forward.\n3. Removing ghost jobs. I discovered that a strong predictor of if a job is a ghost job is that if it keeps being reposted. I was able to identify reposting by doing a embedding text similarity search for jobs from the same company. If 2 job descriptions overlap too much, I only show the date posted for the *earliest* listing. This allowed me to weed out most ghost jobs simply by using a date filter (for example, excluding any jobs posted over a month ago). In my anecdotal, experience this means that I get a higher response rate for data science jobs compared to LinkedIn or Indeed.\n4. Scraping fresh jobs 3x/day. To ensure that my database is reflective of the company career page, I check each company career page 3x/day. Many career pages do not have rate limits because it is in their best interest to allow web scrapers, which is great. For the few that do, I was able to use a rotating proxy. I use Oxylabs for now, but I've heard good things about ScraperAPI, Crawlera.\n5. Building advanced NLP text filters. After playing with GPT4o-mini API, I realized I could can effectively dump raw job descriptions (in HTML) and ask it to give me back formatted information back in JSON (ex salary, yoe, etc). I used this technique to extract a variety of information, including technical keywords, job industry, required licenses &amp; security clearance, if the company sponsors visa, etc.\n6. Powerful search. Once I had the structured JSON data (containing salary, years of experience, remote status, job title, company name, location, and other relevant fields) from ChatGPT's extraction process, I needed a robust search engine to allow users to query and filter jobs efficiently. I chose Elasticsearch due to its powerful full-text search capabilities, filtering, and aggregation features. My favorite feature with Elasticsearch is that it allows me to do Boolean queries. For instance, I can search for job descriptions with technical keywords of \"Pandas\" or \"R\" (example link [here](https://hiring.cafe/?searchState=%7B%22technologyKeywordsQuery%22%3A%22%5C%22Pandas%5C%22+or+%5C%22R%5C%22+%22%7D)).\n\n# Question for the DS community here\n\nBeyond job search, one thing I'm really excited about this 2.1 million job dataset is to be able to do a yearly or quarterly trend report. For instance, to look at what technical skills are growing in demand. What kinds of cool job trends analyses would you do if you had access to this data.", "full_text": "How I scraped 5.3 million jobs (including 5,335 data science jobs)\n\n**Background**\n\nDuring my PhD in Data Science at Stanford, I got sick and tired of ghost jobs &amp; 3rd party offshore agencies on LinkedIn &amp; Indeed. So I wrote a script that fetches jobs from 30k+ company websites' career pages and uses GPT4o-mini to extract relevant information (ex salary, remote, etc.) from job descriptions. You can use it here: ([HiringCafe](http://hiring.cafe)). Here is a filter for Data science jobs (5,335 and counting). I scrape every company 3x/day, so the results stay fresh if you check back the next day.\n\nYou can follow my progress on r/hiringcafe\n\n**How I built the HiringCafe (from a DS perspective)**\n\n1. I identified company career pages with active job listings. I used the [Apollo.io](http://apollo.io/) to search for companies across various industries, and get their company URLs. To narrow these down, I wrote a web crawler (using Node.js, and a combination of Cheerio + Puppeteer depending on site complexity) to find the career page of the company. I discovered that I could dump the raw HTML and prompt ChatGPT o1-mini to classify (as a binary classification) whether each page contained a job description or not. I thus compiled a list of verified job page if it contains a job description or not. If it contains a job description, I add it to a list and proceed to step 2\n2. Verifying legit companies. This part I had to do manually, but it was crucial that I exclude any recruiting firms, 3rd party offshore agencies, etc. because I wanted only high-quality companies directly hiring for roles at their firm. I manually sorted through the 30,000 company career pages (this took several weeks) and picked the ones that looked legit. At Stanford, we call this technique \"occular regression\" :) It was doable because I only had to verify each company a single time and then I trust it moving forward.\n3. Removing ghost jobs. I discovered that a strong predictor of if a job is a ghost job is that if it keeps being reposted. I was able to identify reposting by doing a embedding text similarity search for jobs from the same company. If 2 job descriptions overlap too much, I only show the date posted for the *earliest* listing. This allowed me to weed out most ghost jobs simply by using a date filter (for example, excluding any jobs posted over a month ago). In my anecdotal, experience this means that I get a higher response rate for data science jobs compared to LinkedIn or Indeed.\n4. Scraping fresh jobs 3x/day. To ensure that my database is reflective of the company career page, I check each company career page 3x/day. Many career pages do not have rate limits because it is in their best interest to allow web scrapers, which is great. For the few that do, I was able to use a rotating proxy. I use Oxylabs for now, but I've heard good things about ScraperAPI, Crawlera.\n5. Building advanced NLP text filters. After playing with GPT4o-mini API, I realized I could can effectively dump raw job descriptions (in HTML) and ask it to give me back formatted information back in JSON (ex salary, yoe, etc). I used this technique to extract a variety of information, including technical keywords, job industry, required licenses &amp; security clearance, if the company sponsors visa, etc.\n6. Powerful search. Once I had the structured JSON data (containing salary, years of experience, remote status, job title, company name, location, and other relevant fields) from ChatGPT's extraction process, I needed a robust search engine to allow users to query and filter jobs efficiently. I chose Elasticsearch due to its powerful full-text search capabilities, filtering, and aggregation features. My favorite feature with Elasticsearch is that it allows me to do Boolean queries. For instance, I can search for job descriptions with technical keywords of \"Pandas\" or \"R\" (example link [here](https://hiring.cafe/?searchState=%7B%22technologyKeywordsQuery%22%3A%22%5C%22Pandas%5C%22+or+%5C%22R%5C%22+%22%7D)).\n\n# Question for the DS community here\n\nBeyond job search, one thing I'm really excited about this 2.1 million job dataset is to be able to do a yearly or quarterly trend report. For instance, to look at what technical skills are growing in demand. What kinds of cool job trends analyses would you do if you had access to this data.", "author": "hamed_n", "permalink": "/r/datascience/comments/1qy7a89/how_i_scraped_53_million_jobs_including_5335_data/", "url": "https://www.reddit.com/r/datascience/comments/1qy7a89/how_i_scraped_53_million_jobs_including_5335_data/", "score": 685, "num_comments": 88, "upvote_ratio": 0.93, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qxv8ng", "created_utc": 1770415038.0, "title": "This was posted by a guy who \"helps people get hired\", so take it with a grain of salt - \"Which companies hire the most first-time Data Analysts?\"", "selftext": "", "full_text": "This was posted by a guy who \"helps people get hired\", so take it with a grain of salt - \"Which companies hire the most first-time Data Analysts?\"", "author": "turbo_golf", "permalink": "/r/datascience/comments/1qxv8ng/this_was_posted_by_a_guy_who_helps_people_get/", "url": "https://imgur.com/3v39lf4", "score": 13, "num_comments": 8, "upvote_ratio": 0.73, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qxo3le", "created_utc": 1770399186.0, "title": "easy_sm - A Unix-style CLI for AWS SageMaker that lets you prototype locally before deploying", "selftext": "I built [`easy_sm`](https://prteek.github.io/easy_sm/) to solve a pain point with AWS SageMaker: the slow feedback loop between local development and cloud deployment.\n\n**What it does:**\n\nTrain, process, and deploy ML models locally in Docker containers that mimic SageMaker's environment, then deploy the same code to actual SageMaker with minimal config changes. It also manages endpoints and training jobs with composable, pipable commands following Unix philosophy.\n\n**Why it's useful:**\n\nTest your entire ML workflow locally before spending money on cloud resources. Commands are designed to be chained together, so you can automate common workflows like \"get latest training job → extract model → deploy endpoint\" in a single line.\n\nIt's experimental (APIs may change), requires Python 3.13+, and borrows heavily from [Sagify](https://github.com/Kenza-AI/sagify). MIT licensed.\n\nDocs: [https://prteek.github.io/easy\\_sm/](https://prteek.github.io/easy_sm/)  \nGitHub: [https://github.com/prteek/easy\\_sm](https://github.com/prteek/easy_sm)  \nPyPI: [https://pypi.org/project/easy-sm/](https://pypi.org/project/easy-sm/)\n\nWould love feedback, especially if you've wrestled with SageMaker workflows before.", "full_text": "easy_sm - A Unix-style CLI for AWS SageMaker that lets you prototype locally before deploying\n\nI built [`easy_sm`](https://prteek.github.io/easy_sm/) to solve a pain point with AWS SageMaker: the slow feedback loop between local development and cloud deployment.\n\n**What it does:**\n\nTrain, process, and deploy ML models locally in Docker containers that mimic SageMaker's environment, then deploy the same code to actual SageMaker with minimal config changes. It also manages endpoints and training jobs with composable, pipable commands following Unix philosophy.\n\n**Why it's useful:**\n\nTest your entire ML workflow locally before spending money on cloud resources. Commands are designed to be chained together, so you can automate common workflows like \"get latest training job → extract model → deploy endpoint\" in a single line.\n\nIt's experimental (APIs may change), requires Python 3.13+, and borrows heavily from [Sagify](https://github.com/Kenza-AI/sagify). MIT licensed.\n\nDocs: [https://prteek.github.io/easy\\_sm/](https://prteek.github.io/easy_sm/)  \nGitHub: [https://github.com/prteek/easy\\_sm](https://github.com/prteek/easy_sm)  \nPyPI: [https://pypi.org/project/easy-sm/](https://pypi.org/project/easy-sm/)\n\nWould love feedback, especially if you've wrestled with SageMaker workflows before.", "author": "Far-Media3683", "permalink": "/r/datascience/comments/1qxo3le/easy_sm_a_unixstyle_cli_for_aws_sagemaker_that/", "url": "https://www.reddit.com/r/datascience/comments/1qxo3le/easy_sm_a_unixstyle_cli_for_aws_sagemaker_that/", "score": 3, "num_comments": 1, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qxltyk", "created_utc": 1770394296.0, "title": "Finding myself disillusioned with the quality of discussion in this sub", "selftext": "I see multiple highly-upvoted comments per day saying things like “LLMs aren’t AI,” demonstrating a complete misunderstanding of the technical definitions of these terms. Or worse, comments that say “this stuff isn’t AI, AI is like \\*insert sci-fi reference\\*.” And this is just comments on very high-level topics. If these views are not just being expressed, but are widely upvoted, I can’t help but think this sub is being infiltrated by laypeople without any background in this field and watering down the views of the knowledgeable DS community. I’m wondering if others are feeling this way.\n\nEdits to address some common replies:\n\n* I misspoke about \"the technical definition\" of AI. As others have pointed out, there is no single accepted definition for artificial intelligence.\n* It  is widely accepted in the field that machine learning is a subfield of artificial intelligence.\n   * In the 4th Edition of Russell and Norvig's Artificial Intelligence: A Modern Approach (one of the, if not the, most popular academic texts on the topic) states\n\n&gt;In the public eye, there is sometimes confusion between the terms “artificial intelligence” and “machine learning.” Machine learning is a subfield of AI that studies the ability to improve performance based on experience. Some AI systems use machine learning methods to achieve competence, but some do not.\n\n* My point isn't that everyone who visits this community should know this information. Newcomers and outsiders should be welcome. Comments such as \"LLMs aren’t AI\" indicate that people are confidently posting views that directly contradict widely accepted views within the field. If such easily refutable claims are being confidently shared and upvoted, that indicates to me that more nuanced conversations in this community may be driven by confident yet uninformed opinions. None of us are experts in everything, and, when reading about a topic I don't know much about, I have to trust that others in that conversation are informed. If this community is the blind leading the blind, it is completely worthless.", "full_text": "Finding myself disillusioned with the quality of discussion in this sub\n\nI see multiple highly-upvoted comments per day saying things like “LLMs aren’t AI,” demonstrating a complete misunderstanding of the technical definitions of these terms. Or worse, comments that say “this stuff isn’t AI, AI is like \\*insert sci-fi reference\\*.” And this is just comments on very high-level topics. If these views are not just being expressed, but are widely upvoted, I can’t help but think this sub is being infiltrated by laypeople without any background in this field and watering down the views of the knowledgeable DS community. I’m wondering if others are feeling this way.\n\nEdits to address some common replies:\n\n* I misspoke about \"the technical definition\" of AI. As others have pointed out, there is no single accepted definition for artificial intelligence.\n* It  is widely accepted in the field that machine learning is a subfield of artificial intelligence.\n   * In the 4th Edition of Russell and Norvig's Artificial Intelligence: A Modern Approach (one of the, if not the, most popular academic texts on the topic) states\n\n&gt;In the public eye, there is sometimes confusion between the terms “artificial intelligence” and “machine learning.” Machine learning is a subfield of AI that studies the ability to improve performance based on experience. Some AI systems use machine learning methods to achieve competence, but some do not.\n\n* My point isn't that everyone who visits this community should know this information. Newcomers and outsiders should be welcome. Comments such as \"LLMs aren’t AI\" indicate that people are confidently posting views that directly contradict widely accepted views within the field. If such easily refutable claims are being confidently shared and upvoted, that indicates to me that more nuanced conversations in this community may be driven by confident yet uninformed opinions. None of us are experts in everything, and, when reading about a topic I don't know much about, I have to trust that others in that conversation are informed. If this community is the blind leading the blind, it is completely worthless.", "author": "galactictock", "permalink": "/r/datascience/comments/1qxltyk/finding_myself_disillusioned_with_the_quality_of/", "url": "https://www.reddit.com/r/datascience/comments/1qxltyk/finding_myself_disillusioned_with_the_quality_of/", "score": 169, "num_comments": 147, "upvote_ratio": 0.77, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qxjifc", "created_utc": 1770389151.0, "title": "Data cleaning survival guide", "selftext": "In the [first post](https://www.reddit.com/r/datascience/comments/1qsxuaa/why_is_data_cleaning_hard/), I defined data cleaning as **aligning data with reality**, not making it look neat. Here’s the 2nd post on best practices how to make data cleaning less painful and tedious.\n\n# Data cleaning is a loop\n\nMost real projects follow the same cycle:\n\n**Discovery → Investigation → Resolution**\n\nExample (e-commerce): you see random revenue spikes and a model that predicts “too well.” You inspect spike days, find duplicate orders, talk to the payment team, learn they retry events on timeouts, and ingestion sometimes records both. You then dedupe using an event ID (or keep latest status) and add a flag like collapsed\\_from\\_retries for traceability.\n\nIt’s a loop because you rarely uncover all issues upfront.\n\n# When it becomes slow and painful\n\n* Late / incomplete discovery: you fix one issue, then hit another later, rerun everything, repeat.\n* Cross-team dependency: business and IT don’t prioritize “weird data” until you show impact.\n* Context loss: long cycles, team rotation, meetings, and you end up re-explaining the same story.\n\n# Best practices that actually help\n\n**1) Improve Discovery (find issues earlier)**\n\nTwo common misconceptions:\n\n* exploration isn’t just describe() and null rates, it’s “does this behave like the real system?”\n* discovery isn’t only the data team’s job, you need business/system owners to validate what’s plausible\n\nA simple repeatable approach:\n\n* quick first pass (formats, samples, basic stats)\n* write a small list of **project-critical assumptions** (e.g., “1 row = 1 order”, “timestamps are UTC”)\n* test assumptions with targeted checks\n* validate fast with the people who own the system\n\n**2) Make Investigation manageable**\n\nTreat anomalies like product work:\n\n* prioritize by **impact vs cost** (with the people who will help you).\n* frame issues as outcomes, not complaints (“if we fix this, the churn model improves”)\n* track a small backlog: observation → hypothesis → owner → expected impact → effort\n\n**3) Resolution without destroying signals**\n\n* keep **raw data immutable** (cleaned data is an interpretation layer)\n* implement transformations **by issue** (e.g., resolve\\_gateway\\_retries()), not generic “cleaning steps”, not by column.\n* preserve uncertainty with flags (was\\_imputed, rejection reasons, dedupe indicators)\n\n**Bonus**: documentation is leverage (especially with AI tools)\n\nDon’t just document code. Document **assumptions and decisions** (“negative amounts are refunds, not errors”). Keep a short living “cleaning report” so the loop gets cheaper over time.", "full_text": "Data cleaning survival guide\n\nIn the [first post](https://www.reddit.com/r/datascience/comments/1qsxuaa/why_is_data_cleaning_hard/), I defined data cleaning as **aligning data with reality**, not making it look neat. Here’s the 2nd post on best practices how to make data cleaning less painful and tedious.\n\n# Data cleaning is a loop\n\nMost real projects follow the same cycle:\n\n**Discovery → Investigation → Resolution**\n\nExample (e-commerce): you see random revenue spikes and a model that predicts “too well.” You inspect spike days, find duplicate orders, talk to the payment team, learn they retry events on timeouts, and ingestion sometimes records both. You then dedupe using an event ID (or keep latest status) and add a flag like collapsed\\_from\\_retries for traceability.\n\nIt’s a loop because you rarely uncover all issues upfront.\n\n# When it becomes slow and painful\n\n* Late / incomplete discovery: you fix one issue, then hit another later, rerun everything, repeat.\n* Cross-team dependency: business and IT don’t prioritize “weird data” until you show impact.\n* Context loss: long cycles, team rotation, meetings, and you end up re-explaining the same story.\n\n# Best practices that actually help\n\n**1) Improve Discovery (find issues earlier)**\n\nTwo common misconceptions:\n\n* exploration isn’t just describe() and null rates, it’s “does this behave like the real system?”\n* discovery isn’t only the data team’s job, you need business/system owners to validate what’s plausible\n\nA simple repeatable approach:\n\n* quick first pass (formats, samples, basic stats)\n* write a small list of **project-critical assumptions** (e.g., “1 row = 1 order”, “timestamps are UTC”)\n* test assumptions with targeted checks\n* validate fast with the people who own the system\n\n**2) Make Investigation manageable**\n\nTreat anomalies like product work:\n\n* prioritize by **impact vs cost** (with the people who will help you).\n* frame issues as outcomes, not complaints (“if we fix this, the churn model improves”)\n* track a small backlog: observation → hypothesis → owner → expected impact → effort\n\n**3) Resolution without destroying signals**\n\n* keep **raw data immutable** (cleaned data is an interpretation layer)\n* implement transformations **by issue** (e.g., resolve\\_gateway\\_retries()), not generic “cleaning steps”, not by column.\n* preserve uncertainty with flags (was\\_imputed, rejection reasons, dedupe indicators)\n\n**Bonus**: documentation is leverage (especially with AI tools)\n\nDon’t just document code. Document **assumptions and decisions** (“negative amounts are refunds, not errors”). Keep a short living “cleaning report” so the loop gets cheaper over time.", "author": "SummerElectrical3642", "permalink": "/r/datascience/comments/1qxjifc/data_cleaning_survival_guide/", "url": "https://www.reddit.com/r/datascience/comments/1qxjifc/data_cleaning_survival_guide/", "score": 14, "num_comments": 3, "upvote_ratio": 0.65, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qxf2xt", "created_utc": 1770377129.0, "title": "Is Gen AI the only way forward?", "selftext": "I just had 3 shitty interviews back-to-back. Primarily because there was an insane mismatch between their requirements and my skillset.\n\nI am your standard Data Scientist (*Banking, FMCG and Supply Chain*), with analytics heavy experience along with some ML model development. A generalist, one might say. \n\nI am looking for new jobs but all I get calls are for Gen AI. But their JD mentions other stuff - Relational DBs, Cloud, Standard ML toolkit...you get it. So, I had assumed GenAI would not be the primary requirement, but something like good-to-have.\n\nBut upon facing the interview, it turns out, **these are GenAI developer roles** that require heavily technical and training of LLM models. Oh, these are all API calling companies, not R&amp;D.\n\nClearly, I am not a good fit. But I am unable to get roles/calls in standard business facing data science roles. This kind of indicates the following things:\n\n1. Gen AI is wayyy too much in demand, inspite of all the AI Hype.\n2. The DS boom in last decade has an oversupply of generalists like me, thus standard roles are saturated.\n\n**I would like to know your opinions and definitely can use some advice.**\n\n**Note**: The experience is APAC-specific. I am aware, market in US/Europe is competitive in a whole different manner.", "full_text": "Is Gen AI the only way forward?\n\nI just had 3 shitty interviews back-to-back. Primarily because there was an insane mismatch between their requirements and my skillset.\n\nI am your standard Data Scientist (*Banking, FMCG and Supply Chain*), with analytics heavy experience along with some ML model development. A generalist, one might say. \n\nI am looking for new jobs but all I get calls are for Gen AI. But their JD mentions other stuff - Relational DBs, Cloud, Standard ML toolkit...you get it. So, I had assumed GenAI would not be the primary requirement, but something like good-to-have.\n\nBut upon facing the interview, it turns out, **these are GenAI developer roles** that require heavily technical and training of LLM models. Oh, these are all API calling companies, not R&amp;D.\n\nClearly, I am not a good fit. But I am unable to get roles/calls in standard business facing data science roles. This kind of indicates the following things:\n\n1. Gen AI is wayyy too much in demand, inspite of all the AI Hype.\n2. The DS boom in last decade has an oversupply of generalists like me, thus standard roles are saturated.\n\n**I would like to know your opinions and definitely can use some advice.**\n\n**Note**: The experience is APAC-specific. I am aware, market in US/Europe is competitive in a whole different manner.", "author": "JayBong2k", "permalink": "/r/datascience/comments/1qxf2xt/is_gen_ai_the_only_way_forward/", "url": "https://www.reddit.com/r/datascience/comments/1qxf2xt/is_gen_ai_the_only_way_forward/", "score": 266, "num_comments": 143, "upvote_ratio": 0.88, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qxcgd1", "created_utc": 1770367567.0, "title": "Fun matplotlib upgrade", "selftext": "", "full_text": "Fun matplotlib upgrade", "author": "cantdutchthis", "permalink": "/r/datascience/comments/1qxcgd1/fun_matplotlib_upgrade/", "url": "https://i.redd.it/hehy4p02v2fg1.gif", "score": 187, "num_comments": 20, "upvote_ratio": 0.97, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qx1cr3", "created_utc": 1770334269.0, "title": "Has anyone experienced a hands-on Python coding interview focused on data analysis and model training?", "selftext": "I have a Python coding round coming up where I will need to analyze data, train a model, and evaluate it. I do this for work, so I am confident I can put together a simple model in 60 minutes, but I am not sure how they plan to test Python specifically. Any tips on how to prep for this would be appreciated.", "full_text": "Has anyone experienced a hands-on Python coding interview focused on data analysis and model training?\n\nI have a Python coding round coming up where I will need to analyze data, train a model, and evaluate it. I do this for work, so I am confident I can put together a simple model in 60 minutes, but I am not sure how they plan to test Python specifically. Any tips on how to prep for this would be appreciated.", "author": "Lamp_Shade_Head", "permalink": "/r/datascience/comments/1qx1cr3/has_anyone_experienced_a_handson_python_coding/", "url": "https://www.reddit.com/r/datascience/comments/1qx1cr3/has_anyone_experienced_a_handson_python_coding/", "score": 60, "num_comments": 29, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qx11ri", "created_utc": 1770333508.0, "title": "Traditional ML vs Experimentation Data Scientist", "selftext": "I’m a Senior Data Scientist (5+ years) currently working with traditional ML (forecasting, fraud, pricing) at a large, stable tech company.\n\nI have the option to move to a smaller / startup-like environment focused on causal inference, experimentation (A/B testing, uplift), and Media Mix Modeling (MMM).\n\nI’d really like to hear opinions from people who have experience in either (or both) paths:\n\n\t•\tTraditional ML (predictive models, production systems)\n\n\t•\tCausal inference / experimentation / MMM\n\nSpecifically, I’m curious about your perspective on:\n\n\t1.\tFuture outlook:\n\nWhich path do you think will be more valuable in 5–10 years? Is traditional ML becoming commoditized compared to causal/decision-focused roles?\n\n\t2.\tFinancial return:\n\nIn your experience (especially in the US / Europe / remote roles), which path tends to have higher compensation ceilings at senior/staff levels?\n\n\t3.\tStress vs reward:\n\nHow do these paths compare in day-to-day stress?\n\n(firefighting, on-call, production issues vs ambiguity, stakeholder pressure, politics)\n\n\t4.\tImpact and influence:\n\nWhich roles give you more influence on business decisions and strategy over time?\n\nI’m not early career anymore, so I’m thinking less about “what’s hot right now” and more about long-term leverage, sustainability, and meaningful impact.\n\nAny honest takes, war stories, or regrets are very welcome.", "full_text": "Traditional ML vs Experimentation Data Scientist\n\nI’m a Senior Data Scientist (5+ years) currently working with traditional ML (forecasting, fraud, pricing) at a large, stable tech company.\n\nI have the option to move to a smaller / startup-like environment focused on causal inference, experimentation (A/B testing, uplift), and Media Mix Modeling (MMM).\n\nI’d really like to hear opinions from people who have experience in either (or both) paths:\n\n\t•\tTraditional ML (predictive models, production systems)\n\n\t•\tCausal inference / experimentation / MMM\n\nSpecifically, I’m curious about your perspective on:\n\n\t1.\tFuture outlook:\n\nWhich path do you think will be more valuable in 5–10 years? Is traditional ML becoming commoditized compared to causal/decision-focused roles?\n\n\t2.\tFinancial return:\n\nIn your experience (especially in the US / Europe / remote roles), which path tends to have higher compensation ceilings at senior/staff levels?\n\n\t3.\tStress vs reward:\n\nHow do these paths compare in day-to-day stress?\n\n(firefighting, on-call, production issues vs ambiguity, stakeholder pressure, politics)\n\n\t4.\tImpact and influence:\n\nWhich roles give you more influence on business decisions and strategy over time?\n\nI’m not early career anymore, so I’m thinking less about “what’s hot right now” and more about long-term leverage, sustainability, and meaningful impact.\n\nAny honest takes, war stories, or regrets are very welcome.", "author": "PrestigiousCase5089", "permalink": "/r/datascience/comments/1qx11ri/traditional_ml_vs_experimentation_data_scientist/", "url": "https://www.reddit.com/r/datascience/comments/1qx11ri/traditional_ml_vs_experimentation_data_scientist/", "score": 73, "num_comments": 34, "upvote_ratio": 0.97, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qwz1yi", "created_utc": 1770328758.0, "title": "Writing good evals is brutally hard - so I built an AI to make it easier", "selftext": "I spent years on Apple's Photos ML team teaching models incredibly subjective things - like which photos are \"meaningful\" or \"aesthetic\". It was humbling. Even with careful process, getting consistent evaluation criteria was brutally hard.\n\nNow I build an eval tool called [Kiln](https://github.com/kiln-ai/kiln), and I see others hitting the exact same wall: people can't seem to write great evals. They miss edge cases. They write conflicting requirements. They fail to describe boundary cases clearly. Even when they follow the right process - golden datasets, comparing judge prompts - they struggle to write prompts that LLMs can consistently judge.\n\nSo I built an AI copilot that helps you build evals and synthetic datasets. The result: **5x faster development time and 4x lower judge error rates**.\n\n**TL;DR:** An AI-guided refinement loop that generates tough edge cases, has you compare your judgment to the AI judge, and refines the eval when you disagree. You just rate examples and tell it why it's wrong. Completely free.\n\n# How It Works: AI-Guided Refinement\n\nThe core idea is simple: the AI generates synthetic examples targeting your eval's weak spots. You rate them, tell it why it's wrong when it's wrong, and iterate until aligned.\n\n1. **Review before you build** \\- The AI analyzes your eval goals and task definition before you spend hours labeling. Are there conflicting requirements? Missing details? What does that vague phrase actually mean? It asks clarifying questions upfront.\n2. **Generate tough edge cases** \\- It creates synthetic examples that intentionally probe the boundaries - the cases where your eval criteria are most likely to be unclear or conflicting.\n3. **Compare your judgment to the judge** \\- You see the examples, rate them yourself, and see how the AI judge rated them. When you disagree, you tell it why in plain English. That feedback gets incorporated into the next iteration.\n4. **Iterate until aligned** \\- The loop keeps surfacing cases where you and the judge might disagree, refining the prompts and few-shot examples until the judge matches your intent. If your eval is already solid, you're done in minutes. If it's underspecified, you'll know exactly where.\n\nBy the end, you have an eval dataset, a training dataset, and a synthetic data generation system you can reuse.\n\n# Results\n\nI thought I was decent at writing evals (I build an open-source eval framework). But the evals I create with this system are noticeably better.\n\nFor **technical evals**: it breaks down every edge case, creates clear rule hierarchies, and eliminates conflicting guidance.\n\nFor **subjective evals**: it finds more precise, judgeable language for vague concepts. I said \"no bad jokes\" and it created categories like \"groaner\" and \"cringe\" - specific enough for an LLM to actually judge consistently. Then it builds few-shot examples demonstrating the boundaries.\n\n# Try It\n\nCompletely free and open source. Takes a few minutes to get started:\n\n* [GitHub (4.6k stars)](https://github.com/kiln-ai/kiln)\n* [Docs with Demo](https://docs.kiln.tech/docs/evals-and-specs/specifications)\n\nWhat's the hardest eval you've tried to write? I'm curious what edge cases trip people up - happy to answer questions!", "full_text": "Writing good evals is brutally hard - so I built an AI to make it easier\n\nI spent years on Apple's Photos ML team teaching models incredibly subjective things - like which photos are \"meaningful\" or \"aesthetic\". It was humbling. Even with careful process, getting consistent evaluation criteria was brutally hard.\n\nNow I build an eval tool called [Kiln](https://github.com/kiln-ai/kiln), and I see others hitting the exact same wall: people can't seem to write great evals. They miss edge cases. They write conflicting requirements. They fail to describe boundary cases clearly. Even when they follow the right process - golden datasets, comparing judge prompts - they struggle to write prompts that LLMs can consistently judge.\n\nSo I built an AI copilot that helps you build evals and synthetic datasets. The result: **5x faster development time and 4x lower judge error rates**.\n\n**TL;DR:** An AI-guided refinement loop that generates tough edge cases, has you compare your judgment to the AI judge, and refines the eval when you disagree. You just rate examples and tell it why it's wrong. Completely free.\n\n# How It Works: AI-Guided Refinement\n\nThe core idea is simple: the AI generates synthetic examples targeting your eval's weak spots. You rate them, tell it why it's wrong when it's wrong, and iterate until aligned.\n\n1. **Review before you build** \\- The AI analyzes your eval goals and task definition before you spend hours labeling. Are there conflicting requirements? Missing details? What does that vague phrase actually mean? It asks clarifying questions upfront.\n2. **Generate tough edge cases** \\- It creates synthetic examples that intentionally probe the boundaries - the cases where your eval criteria are most likely to be unclear or conflicting.\n3. **Compare your judgment to the judge** \\- You see the examples, rate them yourself, and see how the AI judge rated them. When you disagree, you tell it why in plain English. That feedback gets incorporated into the next iteration.\n4. **Iterate until aligned** \\- The loop keeps surfacing cases where you and the judge might disagree, refining the prompts and few-shot examples until the judge matches your intent. If your eval is already solid, you're done in minutes. If it's underspecified, you'll know exactly where.\n\nBy the end, you have an eval dataset, a training dataset, and a synthetic data generation system you can reuse.\n\n# Results\n\nI thought I was decent at writing evals (I build an open-source eval framework). But the evals I create with this system are noticeably better.\n\nFor **technical evals**: it breaks down every edge case, creates clear rule hierarchies, and eliminates conflicting guidance.\n\nFor **subjective evals**: it finds more precise, judgeable language for vague concepts. I said \"no bad jokes\" and it created categories like \"groaner\" and \"cringe\" - specific enough for an LLM to actually judge consistently. Then it builds few-shot examples demonstrating the boundaries.\n\n# Try It\n\nCompletely free and open source. Takes a few minutes to get started:\n\n* [GitHub (4.6k stars)](https://github.com/kiln-ai/kiln)\n* [Docs with Demo](https://docs.kiln.tech/docs/evals-and-specs/specifications)\n\nWhat's the hardest eval you've tried to write? I'm curious what edge cases trip people up - happy to answer questions!", "author": "davernow", "permalink": "/r/datascience/comments/1qwz1yi/writing_good_evals_is_brutally_hard_so_i_built_an/", "url": "https://www.reddit.com/r/datascience/comments/1qwz1yi/writing_good_evals_is_brutally_hard_so_i_built_an/", "score": 0, "num_comments": 7, "upvote_ratio": 0.38, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qwcdb6", "created_utc": 1770268237.0, "title": "Thinking About Going into Consulting? McKinsey and BCG Interviews Now Test AI Skills, Too", "selftext": "", "full_text": "Thinking About Going into Consulting? McKinsey and BCG Interviews Now Test AI Skills, Too", "author": "CryoSchema", "permalink": "/r/datascience/comments/1qwcdb6/thinking_about_going_into_consulting_mckinsey_and/", "url": "https://www.interviewquery.com/p/mckinsey-bcg-ai-consulting-interviews", "score": 38, "num_comments": 4, "upvote_ratio": 0.79, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qw9fvl", "created_utc": 1770259970.0, "title": "Production patterns for RAG chatbots: asyncio.gather(), BackgroundTasks, and more", "selftext": "", "full_text": "Production patterns for RAG chatbots: asyncio.gather(), BackgroundTasks, and more", "author": "purposefulCA", "permalink": "/r/datascience/comments/1qw9fvl/production_patterns_for_rag_chatbots/", "url": "/r/Rag/comments/1qw9dxy/production_patterns_for_rag_chatbots/", "score": 9, "num_comments": 0, "upvote_ratio": 0.78, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qvdw7t", "created_utc": 1770176782.0, "title": "Destroy my A/B Test Visualization (Part 2) [D]", "selftext": "", "full_text": "Destroy my A/B Test Visualization (Part 2) [D]", "author": "SingerEast1469", "permalink": "/r/datascience/comments/1qvdw7t/destroy_my_ab_test_visualization_part_2_d/", "url": "/r/statistics/comments/1qv1a2n/destroy_my_ab_test_visualization_part_2_d/", "score": 0, "num_comments": 2, "upvote_ratio": 0.5, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qv95en", "created_utc": 1770164230.0, "title": "Why is backward elimination looked down upon yet my team uses it and the model generates millions?", "selftext": "I’ve been reading Frank Harrell’s critiques of backward elimination, and his arguments make a lot of sense to me.\n\nThat said, if the method is really that problematic, why does it still seem to work reasonably well in practice? My team uses backward elimination regularly for variable selection, and when I pushed back on it, the main justification I got was basically “we only want statistically significant variables.”\n\nAm I missing something here? When, if ever, is backward elimination actually defensible?", "full_text": "Why is backward elimination looked down upon yet my team uses it and the model generates millions?\n\nI’ve been reading Frank Harrell’s critiques of backward elimination, and his arguments make a lot of sense to me.\n\nThat said, if the method is really that problematic, why does it still seem to work reasonably well in practice? My team uses backward elimination regularly for variable selection, and when I pushed back on it, the main justification I got was basically “we only want statistically significant variables.”\n\nAm I missing something here? When, if ever, is backward elimination actually defensible?", "author": "Fig_Towel_379", "permalink": "/r/datascience/comments/1qv95en/why_is_backward_elimination_looked_down_upon_yet/", "url": "https://www.reddit.com/r/datascience/comments/1qv95en/why_is_backward_elimination_looked_down_upon_yet/", "score": 123, "num_comments": 59, "upvote_ratio": 0.89, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qv64eu", "created_utc": 1770156976.0, "title": "First data science coop - should I be wary of this role?", "selftext": "Here is one of my offers:\n\nDetails:\n\n\\- The main project I would work on is demand forecasting which will inform decisions to allocate company resources. I don't actually have systematic time series knowledge as of right now. I do know high level concepts though.\n\n\\- I'd basically be the only real data scientist there. There's no mentor or senior to sanity-check with. there's an MLE but they joined only recently too\n\n\\- I was more knowledgeable than the manager about ML stuff during the interview\n\n\\- There's no return offer with a formal 'data scientist' title.\n\nMy biggest fear is that I'd have to carry everything and own all responsibility and accountability if I take this job. Thoughts?", "full_text": "First data science coop - should I be wary of this role?\n\nHere is one of my offers:\n\nDetails:\n\n\\- The main project I would work on is demand forecasting which will inform decisions to allocate company resources. I don't actually have systematic time series knowledge as of right now. I do know high level concepts though.\n\n\\- I'd basically be the only real data scientist there. There's no mentor or senior to sanity-check with. there's an MLE but they joined only recently too\n\n\\- I was more knowledgeable than the manager about ML stuff during the interview\n\n\\- There's no return offer with a formal 'data scientist' title.\n\nMy biggest fear is that I'd have to carry everything and own all responsibility and accountability if I take this job. Thoughts?", "author": "averagebear_003", "permalink": "/r/datascience/comments/1qv64eu/first_data_science_coop_should_i_be_wary_of_this/", "url": "https://www.reddit.com/r/datascience/comments/1qv64eu/first_data_science_coop_should_i_be_wary_of_this/", "score": 40, "num_comments": 30, "upvote_ratio": 0.93, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qtzy39", "created_utc": 1770050122.0, "title": "U.S. Tech Jobs Could See Growth in Q1 2026, Toptal Data Suggests", "selftext": "", "full_text": "U.S. Tech Jobs Could See Growth in Q1 2026, Toptal Data Suggests", "author": "warmeggnog", "permalink": "/r/datascience/comments/1qtzy39/us_tech_jobs_could_see_growth_in_q1_2026_toptal/", "url": "https://www.interviewquery.com/p/us-tech-jobs-growth-q1-2026", "score": 150, "num_comments": 32, "upvote_ratio": 0.9, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qtzq0k", "created_utc": 1770049646.0, "title": "[Discussion] How many years out are we from this?", "selftext": "", "full_text": "[Discussion] How many years out are we from this?", "author": "protonchase", "permalink": "/r/datascience/comments/1qtzq0k/discussion_how_many_years_out_are_we_from_this/", "url": "/r/statistics/comments/1qtzpgv/discussion_how_many_years_out_are_we_from_this/", "score": 0, "num_comments": 14, "upvote_ratio": 0.27, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qtr5cw", "created_utc": 1770026913.0, "title": "[Project] PerpetualBooster v1.1.2: GBM without hyperparameter tuning, now 2x faster with ONNX/XGBoost support", "selftext": "Hi all,\n\nWe just released v1.1.2 of PerpetualBooster. For those who haven't seen it, it's a gradient boosting machine (GBM) written in Rust that eliminates the need for hyperparameter optimization by using a generalization algorithm controlled by a single \"budget\" parameter.\n\nThis update focuses on performance, stability, and ecosystem integration.\n\nKey Technical Updates:\n- Performance: up to 2x faster training.\n- Ecosystem: Full R release, ONNX support, and native \"Save as XGBoost\" for interoperability.\n- Python Support: Added Python 3.14, dropped 3.9.\n- Data Handling: Zero-copy Polars support (no memory overhead).\n- API Stability: v1.0.0 is now the baseline, with guaranteed backward compatibility for all 1.x.x releases (compatible back to v0.10.0).\n\nBenchmarking against LightGBM + Optuna typically shows a 100x wall-time speedup to reach the same accuracy since it hits the result in a single run.\n\nGitHub: https://github.com/perpetual-ml/perpetual\n\nWould love to hear any feedback or answer questions about the algorithm!\n", "full_text": "[Project] PerpetualBooster v1.1.2: GBM without hyperparameter tuning, now 2x faster with ONNX/XGBoost support\n\nHi all,\n\nWe just released v1.1.2 of PerpetualBooster. For those who haven't seen it, it's a gradient boosting machine (GBM) written in Rust that eliminates the need for hyperparameter optimization by using a generalization algorithm controlled by a single \"budget\" parameter.\n\nThis update focuses on performance, stability, and ecosystem integration.\n\nKey Technical Updates:\n- Performance: up to 2x faster training.\n- Ecosystem: Full R release, ONNX support, and native \"Save as XGBoost\" for interoperability.\n- Python Support: Added Python 3.14, dropped 3.9.\n- Data Handling: Zero-copy Polars support (no memory overhead).\n- API Stability: v1.0.0 is now the baseline, with guaranteed backward compatibility for all 1.x.x releases (compatible back to v0.10.0).\n\nBenchmarking against LightGBM + Optuna typically shows a 100x wall-time speedup to reach the same accuracy since it hits the result in a single run.\n\nGitHub: https://github.com/perpetual-ml/perpetual\n\nWould love to hear any feedback or answer questions about the algorithm!", "author": "mutlu_simsek", "permalink": "/r/datascience/comments/1qtr5cw/project_perpetualbooster_v112_gbm_without/", "url": "https://www.reddit.com/r/datascience/comments/1qtr5cw/project_perpetualbooster_v112_gbm_without/", "score": 78, "num_comments": 17, "upvote_ratio": 0.95, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qtlvfu", "created_utc": 1770008498.0, "title": "Weekly Entering &amp; Transitioning - Thread 02 Feb, 2026 - 09 Feb, 2026", "selftext": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "full_text": "Weekly Entering &amp; Transitioning - Thread 02 Feb, 2026 - 09 Feb, 2026\n\n \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "author": "AutoModerator", "permalink": "/r/datascience/comments/1qtlvfu/weekly_entering_transitioning_thread_02_feb_2026/", "url": "https://www.reddit.com/r/datascience/comments/1qtlvfu/weekly_entering_transitioning_thread_02_feb_2026/", "score": 7, "num_comments": 20, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qt2hhe", "created_utc": 1769962171.0, "title": "My thoughts on my recent interview experiences in tech", "selftext": "Hi folks,\n\nYou might remember me from some of my previous posts in this subreddit about how to pass product analytics interviews in tech.\n\nWell, it turns out I needed to take my own advice because I was laid off last year. I recently started interviewing and wanted to share my experience in case it’s helpful. I also share what I learned about salary and total compensation.\n\nNote that this post is mostly about my experience trying to pass interviews, not about getting interviews.\n\n# Context\n\n* I’m a data scientist focused on product analytics in tech, targeting staff and lead level roles. This post won’t be very relevant to you if you’re more focused on machine learning, data engineering, or research\n* I started applying on January 1st\n* In the last two weeks, I had:\n   * 6 recruiter calls\n   * 4 tech screens\n   * 2 hiring manager calls\n\nCompanies so far are a mix of MAANG, other large tech companies, and mid to late stage startups.\n\n# Pipeline so far:\n\n* 6 recruiter screens\n* 5 moved me forward\n* 4 tech screens, two hiring manager calls (1 hiring manager did not move me forward)\n* I passed 2 tech screens, waiting to hear back from the other 2\n* Right now I have two final rounds coming up. One with a MAANG and one with a startup.\n\n# Recruiter Calls\n\nThe recruiter calls were all pretty similar. They asked me:\n\n* About my background and experience\n* One behavioral question (influencing roadmap, leading an AB test, etc.)\n* What I’m looking for next\n* Compensation expectations\n* Work eligibility and remote or relocation preferences\n* My timeline, where I am in the process with other companies\n* They told me more about the company, role, and what the process looks like\n\n**Here’s a tip about compensation:** I did my research so when they asked my compensation expectations, I told them a number that I thought would be on the high end of their band. But here's the tip: After sharing my number, I asked: “Is that in your range?”\n\nOnce they replied, I followed with: “What is the range, if you don’t mind me asking?”\n\n2 out of 6 recruiters actually shared what typical offers look like!\n\nA MAAANG company told me:\n\n* Staff/Lead: 230k base, 390k total comp, 40k signing bonus\n* Senior: 195k base, 280k total comp, 20k signing bonus\n\nA late stage startup told me: \n\n* Staff/Lead: 235k base, 435k total comp\n* Senior: 200k base, 315k total comp\n* (I don’t know how they’re valuing their equity to come up with total comp)\n\n# Tech Screens\n\nI’ve done 4 tech screens so far. All were 45 to 60 minutes.\n\n**SQL**\n\nAll four tested SQL. I used SQL daily at work, but I was rusty from not working for a while. I used [Stratascratch ](https://www.stratascratch.com/?via=productanalyst)to brush up. I did 5 questions per day for 10 days: 1 easy, 3 medium, 1 hard.\n\nMy rule of thumb for SQL is:\n\n* Easy: 100% in under 3 minutes\n* Medium: 100% in under 4 minutes\n* Hard: \\~80% in under 7 minutes\n\nIf you can do this, you can pass almost any SQL tech screen for product analytics roles.\n\n**Case questions**\n\n3 out of 4 tech screens had some type of case product question.\n\n* Two were follow ups to the SQL. I was asked to interpret the results, explain what is happening, hypothesize why, where I would dig deeper, etc.\n* One asked a standalone case: Is feature X better than feature Y? I had to define what “better” means, propose metrics, outline an AB test\n* One showed me some statistical output and asked me to interpret it, what other data I would want to see, and recommend next steps. The output contained a bunch of descriptive data, a funnel analysis, and p-values\n\nIf you struggle with product sense, analytics case questions, and/or AB testing, there’s a lot of resources out there. Here’s what I used:\n\n* [Here's a free framework and case study](https://medium.com/datainterview/principles-and-frameworks-of-product-metrics-youtube-case-study-ff63257a82d3)\n* [Another framework guide](https://medium.com/data-science/the-ultimate-guide-to-cracking-business-case-interviews-for-data-scientists-part-1-cb768c37edf4)\n* Watch mock interviews on Youtube\n* If you’re willing to spend some money, [Ace the Data Science Interview ](https://amzn.to/4a9kzTE)has a few good chapters with common frameworks, and several practice cases with answers\n* [Trustworthy Online Controlled Experiments](https://amzn.to/4qS2O2p) is the gold standard for AB testing\n\n**Python**\n\nOnly one tech screen so far had a Python component, but another tech screen that I’m waiting to take has a Python component too. I don’t use Python much in my day to day work. I do my data wrangling in SQL and use Python just for statistical tests. And even when I did use Python, I’d lean on AI, so I’m weak on this part. Again, I used [Stratascratch ](https://www.stratascratch.com/?via=productanalyst)to prep. I usually do 5-10 questions a day. But I focused too much on manipulating data with Pandas.\n\nThe one Python tech screen I had tested on:\n\n* Functions\n* Loops\n* List comprehension\n\nI can’t do these from memory so I did not do well in the interview.\n\n# Hiring Manager Calls\n\nI had two of these. Some companies stick this step in between the recruiter screen and tech screen. \n\nI was asked about:\n\n* Specific examples of influencing the roadmap\n* Working with, and influencing leadership\n* Most technical project I’ve worked on\n* One case question about measuring the success of a feature\n* What I’m looking for next\n\n# Where I am now\n\n* Two final rounds scheduled in the next 2-3 weeks\n* Waiting to hear back from two tech screens\n\n# Final thoughts\n\nIt feels like the current job market is much harder than when I was looking \\~4 years ago. It’s harder to get interviews, and the tech screens are harder. When I was looking 4 years ago, I must have done 8 or 10 tech screens and they were purely SQL. Now, the tech screens might have a Python component and case questions.\n\nThe pay bands also seem lower or flat compared to 4 years ago. The Senior total comp at one MAANG is lower than what I was offered in 2022 as a Senior, and the Staff/Lead total comp is lower than what I was making as a Senior in big tech. \n\nI hope this was helpful. I plan to do another update after I do a few final loops. If you want more information about how to pass product analytics interviews at tech companies, check out my previous post: [How to pass the Product Analytics interview at tech companies](https://futureproductanalyst.substack.com/p/how-to-pass-the-product-analytics)", "full_text": "My thoughts on my recent interview experiences in tech\n\nHi folks,\n\nYou might remember me from some of my previous posts in this subreddit about how to pass product analytics interviews in tech.\n\nWell, it turns out I needed to take my own advice because I was laid off last year. I recently started interviewing and wanted to share my experience in case it’s helpful. I also share what I learned about salary and total compensation.\n\nNote that this post is mostly about my experience trying to pass interviews, not about getting interviews.\n\n# Context\n\n* I’m a data scientist focused on product analytics in tech, targeting staff and lead level roles. This post won’t be very relevant to you if you’re more focused on machine learning, data engineering, or research\n* I started applying on January 1st\n* In the last two weeks, I had:\n   * 6 recruiter calls\n   * 4 tech screens\n   * 2 hiring manager calls\n\nCompanies so far are a mix of MAANG, other large tech companies, and mid to late stage startups.\n\n# Pipeline so far:\n\n* 6 recruiter screens\n* 5 moved me forward\n* 4 tech screens, two hiring manager calls (1 hiring manager did not move me forward)\n* I passed 2 tech screens, waiting to hear back from the other 2\n* Right now I have two final rounds coming up. One with a MAANG and one with a startup.\n\n# Recruiter Calls\n\nThe recruiter calls were all pretty similar. They asked me:\n\n* About my background and experience\n* One behavioral question (influencing roadmap, leading an AB test, etc.)\n* What I’m looking for next\n* Compensation expectations\n* Work eligibility and remote or relocation preferences\n* My timeline, where I am in the process with other companies\n* They told me more about the company, role, and what the process looks like\n\n**Here’s a tip about compensation:** I did my research so when they asked my compensation expectations, I told them a number that I thought would be on the high end of their band. But here's the tip: After sharing my number, I asked: “Is that in your range?”\n\nOnce they replied, I followed with: “What is the range, if you don’t mind me asking?”\n\n2 out of 6 recruiters actually shared what typical offers look like!\n\nA MAAANG company told me:\n\n* Staff/Lead: 230k base, 390k total comp, 40k signing bonus\n* Senior: 195k base, 280k total comp, 20k signing bonus\n\nA late stage startup told me: \n\n* Staff/Lead: 235k base, 435k total comp\n* Senior: 200k base, 315k total comp\n* (I don’t know how they’re valuing their equity to come up with total comp)\n\n# Tech Screens\n\nI’ve done 4 tech screens so far. All were 45 to 60 minutes.\n\n**SQL**\n\nAll four tested SQL. I used SQL daily at work, but I was rusty from not working for a while. I used [Stratascratch ](https://www.stratascratch.com/?via=productanalyst)to brush up. I did 5 questions per day for 10 days: 1 easy, 3 medium, 1 hard.\n\nMy rule of thumb for SQL is:\n\n* Easy: 100% in under 3 minutes\n* Medium: 100% in under 4 minutes\n* Hard: \\~80% in under 7 minutes\n\nIf you can do this, you can pass almost any SQL tech screen for product analytics roles.\n\n**Case questions**\n\n3 out of 4 tech screens had some type of case product question.\n\n* Two were follow ups to the SQL. I was asked to interpret the results, explain what is happening, hypothesize why, where I would dig deeper, etc.\n* One asked a standalone case: Is feature X better than feature Y? I had to define what “better” means, propose metrics, outline an AB test\n* One showed me some statistical output and asked me to interpret it, what other data I would want to see, and recommend next steps. The output contained a bunch of descriptive data, a funnel analysis, and p-values\n\nIf you struggle with product sense, analytics case questions, and/or AB testing, there’s a lot of resources out there. Here’s what I used:\n\n* [Here's a free framework and case study](https://medium.com/datainterview/principles-and-frameworks-of-product-metrics-youtube-case-study-ff63257a82d3)\n* [Another framework guide](https://medium.com/data-science/the-ultimate-guide-to-cracking-business-case-interviews-for-data-scientists-part-1-cb768c37edf4)\n* Watch mock interviews on Youtube\n* If you’re willing to spend some money, [Ace the Data Science Interview ](https://amzn.to/4a9kzTE)has a few good chapters with common frameworks, and several practice cases with answers\n* [Trustworthy Online Controlled Experiments](https://amzn.to/4qS2O2p) is the gold standard for AB testing\n\n**Python**\n\nOnly one tech screen so far had a Python component, but another tech screen that I’m waiting to take has a Python component too. I don’t use Python much in my day to day work. I do my data wrangling in SQL and use Python just for statistical tests. And even when I did use Python, I’d lean on AI, so I’m weak on this part. Again, I used [Stratascratch ](https://www.stratascratch.com/?via=productanalyst)to prep. I usually do 5-10 questions a day. But I focused too much on manipulating data with Pandas.\n\nThe one Python tech screen I had tested on:\n\n* Functions\n* Loops\n* List comprehension\n\nI can’t do these from memory so I did not do well in the interview.\n\n# Hiring Manager Calls\n\nI had two of these. Some companies stick this step in between the recruiter screen and tech screen. \n\nI was asked about:\n\n* Specific examples of influencing the roadmap\n* Working with, and influencing leadership\n* Most technical project I’ve worked on\n* One case question about measuring the success of a feature\n* What I’m looking for next\n\n# Where I am now\n\n* Two final rounds scheduled in the next 2-3 weeks\n* Waiting to hear back from two tech screens\n\n# Final thoughts\n\nIt feels like the current job market is much harder than when I was looking \\~4 years ago. It’s harder to get interviews, and the tech screens are harder. When I was looking 4 years ago, I must have done 8 or 10 tech screens and they were purely SQL. Now, the tech screens might have a Python component and case questions.\n\nThe pay bands also seem lower or flat compared to 4 years ago. The Senior total comp at one MAANG is lower than what I was offered in 2022 as a Senior, and the Staff/Lead total comp is lower than what I was making as a Senior in big tech. \n\nI hope this was helpful. I plan to do another update after I do a few final loops. If you want more information about how to pass product analytics interviews at tech companies, check out my previous post: [How to pass the Product Analytics interview at tech companies](https://futureproductanalyst.substack.com/p/how-to-pass-the-product-analytics)", "author": "productanalyst9", "permalink": "/r/datascience/comments/1qt2hhe/my_thoughts_on_my_recent_interview_experiences_in/", "url": "https://www.reddit.com/r/datascience/comments/1qt2hhe/my_thoughts_on_my_recent_interview_experiences_in/", "score": 0, "num_comments": 18, "upvote_ratio": 0.46, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qsylys", "created_utc": 1769952910.0, "title": "Brainstorming around the visualization of customer segment data", "selftext": "", "full_text": "Brainstorming around the visualization of customer segment data", "author": "SingerEast1469", "permalink": "/r/datascience/comments/1qsylys/brainstorming_around_the_visualization_of/", "url": "https://ibb.co/C3pxC8TV", "score": 1, "num_comments": 8, "upvote_ratio": 0.6, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qsxuaa", "created_utc": 1769950805.0, "title": "Why is data cleaning hard?", "selftext": "In almost all polls, data cleaning is always at the top of data scientists’ pain points.\n\nRecently, I tried to sit down and structure my thought about it from first principles.\n\nIt help me realized what actually is data cleaning, why it is often necessary and why it feels hard.\n\n\\- data cleaning is not about make data looks cleaner, it is fixing data to be closer to reality.\n\n\\- data cleaning is often necessary in data science when we work on new use cases, or simply because the data pipeline fail at some point.\n\n\\- data cleaning is hard because it often requires knowledge from other teams: business knowledge from operational team and system knowledge from IT team. This make it slow and painful particularly when those teams are not ready to support data science.\n\nThis is a first article on the topic, I will try to do other articles on best prectices to make the process better and maybe a case study. Hopefully it could help our community, mostly junior ppl.\n\nAnd you, how are your experience and thoughts on this topic?", "full_text": "Why is data cleaning hard?\n\nIn almost all polls, data cleaning is always at the top of data scientists’ pain points.\n\nRecently, I tried to sit down and structure my thought about it from first principles.\n\nIt help me realized what actually is data cleaning, why it is often necessary and why it feels hard.\n\n\\- data cleaning is not about make data looks cleaner, it is fixing data to be closer to reality.\n\n\\- data cleaning is often necessary in data science when we work on new use cases, or simply because the data pipeline fail at some point.\n\n\\- data cleaning is hard because it often requires knowledge from other teams: business knowledge from operational team and system knowledge from IT team. This make it slow and painful particularly when those teams are not ready to support data science.\n\nThis is a first article on the topic, I will try to do other articles on best prectices to make the process better and maybe a case study. Hopefully it could help our community, mostly junior ppl.\n\nAnd you, how are your experience and thoughts on this topic?", "author": "SummerElectrical3642", "permalink": "/r/datascience/comments/1qsxuaa/why_is_data_cleaning_hard/", "url": "https://www.reddit.com/r/datascience/comments/1qsxuaa/why_is_data_cleaning_hard/", "score": 1, "num_comments": 19, "upvote_ratio": 0.51, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qsls5g", "created_utc": 1769912388.0, "title": "Am I drifting away from Data Science, or building useful foundations? (2 YOE working in a startup, no coding)", "selftext": "I’m looking for some career perspective and would really appreciate advice from people working in or around data science. \n\nI’m currently not sure where exactly is my career heading and want to start a business eventually in which I can use my data science skills as a tool, not forcefully but purposefully. \n\nAlso my current job is giving me good experience of being in a startup environment where I’m able to learning to set up a manufacturing facility from scratch and able to first hand see business decisions and strategies. I also have some freedom to implement some of my ideas to improve or set new systems in the company and see it work eg. using m365 tools like sharepoint power automate power apps etc to create portals, apps and automation flows which collect data and I present that in meetings. But this involves no coding at all and very little implementation of what I learnt in school. \n\nRight now I’m struggling with a few questions:\n\n1)Am I moving away from a real data science career, or building underrated foundations?\n\n2)What does an actual data science role look like day-to-day in practice?\n\n3)Is this kind of startup + tooling experience valuable, or will it hurt me later?\n\n4)If my end goal is entrepreneurship + data, what skills should I be prioritizing now?\n\n5)At what point should I consider switching roles or companies?\n\nThis is my first job and I’ve been here for 2 years. I’m not sure what exactly to expect from an actual DS role and currently I’m not sure if Im going in the right direction to achieve my end goal of starting a company of my own before 30s.", "full_text": "Am I drifting away from Data Science, or building useful foundations? (2 YOE working in a startup, no coding)\n\nI’m looking for some career perspective and would really appreciate advice from people working in or around data science. \n\nI’m currently not sure where exactly is my career heading and want to start a business eventually in which I can use my data science skills as a tool, not forcefully but purposefully. \n\nAlso my current job is giving me good experience of being in a startup environment where I’m able to learning to set up a manufacturing facility from scratch and able to first hand see business decisions and strategies. I also have some freedom to implement some of my ideas to improve or set new systems in the company and see it work eg. using m365 tools like sharepoint power automate power apps etc to create portals, apps and automation flows which collect data and I present that in meetings. But this involves no coding at all and very little implementation of what I learnt in school. \n\nRight now I’m struggling with a few questions:\n\n1)Am I moving away from a real data science career, or building underrated foundations?\n\n2)What does an actual data science role look like day-to-day in practice?\n\n3)Is this kind of startup + tooling experience valuable, or will it hurt me later?\n\n4)If my end goal is entrepreneurship + data, what skills should I be prioritizing now?\n\n5)At what point should I consider switching roles or companies?\n\nThis is my first job and I’ve been here for 2 years. I’m not sure what exactly to expect from an actual DS role and currently I’m not sure if Im going in the right direction to achieve my end goal of starting a company of my own before 30s.", "author": "No-System-2838", "permalink": "/r/datascience/comments/1qsls5g/am_i_drifting_away_from_data_science_or_building/", "url": "https://www.reddit.com/r/datascience/comments/1qsls5g/am_i_drifting_away_from_data_science_or_building/", "score": 40, "num_comments": 10, "upvote_ratio": 0.9, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qrtgse", "created_utc": 1769836529.0, "title": "What separates data scientists who earn a good living (100k-200k) from those who earn 300k+ at FAANG?", "selftext": "Is it just stock options and vesting? Or is it just FAANG is a lot of work. Why do some data scientists deserve that much? I work at a Fortune 500 and the ceiling for IC data scientists is around $200k unless you go into management of course. But how and why do people make 500k at Google without going into management? Obviously I’m talking about 1% or less of data scientists but still. I’m less than a year into my full time data scientist job and figuring out my goals and long term plans. ", "full_text": "What separates data scientists who earn a good living (100k-200k) from those who earn 300k+ at FAANG?\n\nIs it just stock options and vesting? Or is it just FAANG is a lot of work. Why do some data scientists deserve that much? I work at a Fortune 500 and the ceiling for IC data scientists is around $200k unless you go into management of course. But how and why do people make 500k at Google without going into management? Obviously I’m talking about 1% or less of data scientists but still. I’m less than a year into my full time data scientist job and figuring out my goals and long term plans.", "author": "Tenet_Bull", "permalink": "/r/datascience/comments/1qrtgse/what_separates_data_scientists_who_earn_a_good/", "url": "https://www.reddit.com/r/datascience/comments/1qrtgse/what_separates_data_scientists_who_earn_a_good/", "score": 546, "num_comments": 207, "upvote_ratio": 0.95, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qrohou", "created_utc": 1769822643.0, "title": "Managers what's your LLM strategy?", "selftext": "I'm a data science manager with a small team, so I've been interested in figuring out how to use more LLM magic to get my team some time back. \n\nWondering what some common strategies are? \n\nThe areas I've found challenges in are \n\n* documentation: we don't have enough detailed documentation readily available to plug in, so it's like a cold start problem. \n\n* validation: LLMs are so eager to spit out lines of code, so it writes 100 lines of code for the 20 lines of code it needed and reviewing it can be almost more effort than writing it yourself. \n\n* tools: either we give it something too generic and have to write a ton of documentation / best practice or we spend a ton of time structuring the tools to the point we lack any flexibility. \n\n\n\n\n\n\n", "full_text": "Managers what's your LLM strategy?\n\nI'm a data science manager with a small team, so I've been interested in figuring out how to use more LLM magic to get my team some time back. \n\nWondering what some common strategies are? \n\nThe areas I've found challenges in are \n\n* documentation: we don't have enough detailed documentation readily available to plug in, so it's like a cold start problem. \n\n* validation: LLMs are so eager to spit out lines of code, so it writes 100 lines of code for the 20 lines of code it needed and reviewing it can be almost more effort than writing it yourself. \n\n* tools: either we give it something too generic and have to write a ton of documentation / best practice or we spend a ton of time structuring the tools to the point we lack any flexibility.", "author": "testtestuser2", "permalink": "/r/datascience/comments/1qrohou/managers_whats_your_llm_strategy/", "url": "https://www.reddit.com/r/datascience/comments/1qrohou/managers_whats_your_llm_strategy/", "score": 31, "num_comments": 27, "upvote_ratio": 0.76, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qqvlcn", "created_utc": 1769747513.0, "title": "While US Tech Hiring Slows, Countries Like Finland Are Attracting AI Talent", "selftext": "", "full_text": "While US Tech Hiring Slows, Countries Like Finland Are Attracting AI Talent", "author": "KitchenTaste7229", "permalink": "/r/datascience/comments/1qqvlcn/while_us_tech_hiring_slows_countries_like_finland/", "url": "https://www.interviewquery.com/p/finland-fast-track-tech-visas-ai-talent", "score": 175, "num_comments": 24, "upvote_ratio": 0.95, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qqtj9y", "created_utc": 1769741701.0, "title": "From Individual Contributor to Team Lead — what actually changes in how you create value?", "selftext": "I recently got promoted from individual contributor to data science team lead, and honestly I’m still trying to recalibrate how I should work and think.\n\nAs an IC, value creation was pretty straightforward: pick a problem, solve it well, ship something useful. If I did my part right, the value was there.\n\nNow as a team lead, the bottleneck feels very different. It’s much more about judgment than execution:\n\n* Is this problem even worth solving?\n* Does it matter for the business or the system as a whole?\n* Is it worth spending our limited time and people on it instead of something else?\n* How do I get results *through* other people and through the organization, rather than by doing everything myself?\n\nI find that being “technically right” is often not the hard part anymore. The harder part is deciding *what* to be right about, and *where* to apply effort.\n\nFor those of you who’ve made a similar transition:\n\n* How did you train your sense of value judgment?\n* How do you decide what *not* to work on?\n* What helped you move from “doing good work yourself” to “creating leverage through others”?\n* Any mental models, habits, or mistakes-you-learned-from that were particularly helpful?\n\nWould love to hear how people here think about this shift. I suspect this is one of those transitions that looks simple from the outside but is actually pretty deep.", "full_text": "From Individual Contributor to Team Lead — what actually changes in how you create value?\n\nI recently got promoted from individual contributor to data science team lead, and honestly I’m still trying to recalibrate how I should work and think.\n\nAs an IC, value creation was pretty straightforward: pick a problem, solve it well, ship something useful. If I did my part right, the value was there.\n\nNow as a team lead, the bottleneck feels very different. It’s much more about judgment than execution:\n\n* Is this problem even worth solving?\n* Does it matter for the business or the system as a whole?\n* Is it worth spending our limited time and people on it instead of something else?\n* How do I get results *through* other people and through the organization, rather than by doing everything myself?\n\nI find that being “technically right” is often not the hard part anymore. The harder part is deciding *what* to be right about, and *where* to apply effort.\n\nFor those of you who’ve made a similar transition:\n\n* How did you train your sense of value judgment?\n* How do you decide what *not* to work on?\n* What helped you move from “doing good work yourself” to “creating leverage through others”?\n* Any mental models, habits, or mistakes-you-learned-from that were particularly helpful?\n\nWould love to hear how people here think about this shift. I suspect this is one of those transitions that looks simple from the outside but is actually pretty deep.", "author": "Rich-Effect2152", "permalink": "/r/datascience/comments/1qqtj9y/from_individual_contributor_to_team_lead_what/", "url": "https://www.reddit.com/r/datascience/comments/1qqtj9y/from_individual_contributor_to_team_lead_what/", "score": 50, "num_comments": 14, "upvote_ratio": 0.91, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qqg341", "created_utc": 1769710000.0, "title": "Just had a job interview and was told that no-one uses Airflow in 2026", "selftext": "So basically the title. I didn't react to the comment because I just was extremely surprised by it. What is your experience? How true is the statement?", "full_text": "Just had a job interview and was told that no-one uses Airflow in 2026\n\nSo basically the title. I didn't react to the comment because I just was extremely surprised by it. What is your experience? How true is the statement?", "author": "xerlivex", "permalink": "/r/datascience/comments/1qqg341/just_had_a_job_interview_and_was_told_that_noone/", "url": "https://www.reddit.com/r/datascience/comments/1qqg341/just_had_a_job_interview_and_was_told_that_noone/", "score": 106, "num_comments": 90, "upvote_ratio": 0.93, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qputs6", "created_utc": 1769650710.0, "title": "Google Maps query for whole state", "selftext": "I live in North Carolina, US and in my state there is a grocery chain called Food Lion. Anecdotally I have observed that where there is a Food Lion there is a Chinese restaurant in the same shopping center. \n\nIs there a way to query Google Maps for Food Lion and Chinese restaurants in the state of North Carolina and get the latitude and longitude for each location so I can calculate all the distances?", "full_text": "Google Maps query for whole state\n\nI live in North Carolina, US and in my state there is a grocery chain called Food Lion. Anecdotally I have observed that where there is a Food Lion there is a Chinese restaurant in the same shopping center. \n\nIs there a way to query Google Maps for Food Lion and Chinese restaurants in the state of North Carolina and get the latitude and longitude for each location so I can calculate all the distances?", "author": "big_data_mike", "permalink": "/r/datascience/comments/1qputs6/google_maps_query_for_whole_state/", "url": "https://www.reddit.com/r/datascience/comments/1qputs6/google_maps_query_for_whole_state/", "score": 41, "num_comments": 10, "upvote_ratio": 0.92, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qohv5a", "created_utc": 1769529732.0, "title": "How long did it take you to get comfortable with statistics?", "selftext": "how long did it take from your first undergrad class to when you felt comfortable with understanding statistics? (Whatever that means for you)\n\nWhen did you get the feeling like you understood the methodologies and papers needed for your level?", "full_text": "How long did it take you to get comfortable with statistics?\n\nhow long did it take from your first undergrad class to when you felt comfortable with understanding statistics? (Whatever that means for you)\n\nWhen did you get the feeling like you understood the methodologies and papers needed for your level?", "author": "LeaguePrototype", "permalink": "/r/datascience/comments/1qohv5a/how_long_did_it_take_you_to_get_comfortable_with/", "url": "https://www.reddit.com/r/datascience/comments/1qohv5a/how_long_did_it_take_you_to_get_comfortable_with/", "score": 71, "num_comments": 51, "upvote_ratio": 0.97, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qnshcs", "created_utc": 1769460577.0, "title": "What do you guys do during a gridsearch", "selftext": "So I'm building some models and I'm having to do some gridsearch to fine tune my decision trees. They take about 50 mins for my computer to run. \n\nI'm just curious what everyone does while these long processes are running. Getting coffee and a conversation is only 10mins. \n\nThanks ", "full_text": "What do you guys do during a gridsearch\n\nSo I'm building some models and I'm having to do some gridsearch to fine tune my decision trees. They take about 50 mins for my computer to run. \n\nI'm just curious what everyone does while these long processes are running. Getting coffee and a conversation is only 10mins. \n\nThanks", "author": "Champagnemusic", "permalink": "/r/datascience/comments/1qnshcs/what_do_you_guys_do_during_a_gridsearch/", "url": "https://www.reddit.com/r/datascience/comments/1qnshcs/what_do_you_guys_do_during_a_gridsearch/", "score": 59, "num_comments": 59, "upvote_ratio": 0.9, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qn6qhu", "created_utc": 1769403688.0, "title": "Weekly Entering &amp; Transitioning - Thread 26 Jan, 2026 - 02 Feb, 2026", "selftext": " \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "full_text": "Weekly Entering &amp; Transitioning - Thread 26 Jan, 2026 - 02 Feb, 2026\n\n \n\nWelcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\n\n* Learning resources (e.g. books, tutorials, videos)\n* Traditional education (e.g. schools, degrees, electives)\n* Alternative education (e.g. online courses, bootcamps)\n* Job search questions (e.g. resumes, applying, career prospects)\n* Elementary questions (e.g. where to start, what next)\n\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).", "author": "AutoModerator", "permalink": "/r/datascience/comments/1qn6qhu/weekly_entering_transitioning_thread_26_jan_2026/", "url": "https://www.reddit.com/r/datascience/comments/1qn6qhu/weekly_entering_transitioning_thread_26_jan_2026/", "score": 14, "num_comments": 19, "upvote_ratio": 0.95, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qlb03x", "created_utc": 1769222518.0, "title": "Went on a date and the girl said... \"Soooo.... What kind of... data do you science???\"", "selftext": "Didn't know what to say. Humor me with your responses.\n\nUpdate: I sent her this post and she loved it 🤣", "full_text": "Went on a date and the girl said... \"Soooo.... What kind of... data do you science???\"\n\nDidn't know what to say. Humor me with your responses.\n\nUpdate: I sent her this post and she loved it 🤣", "author": "Training_Butterfly70", "permalink": "/r/datascience/comments/1qlb03x/went_on_a_date_and_the_girl_said_soooo_what_kind/", "url": "https://www.reddit.com/r/datascience/comments/1qlb03x/went_on_a_date_and_the_girl_said_soooo_what_kind/", "score": 1011, "num_comments": 152, "upvote_ratio": 0.94, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qkzkgd", "created_utc": 1769194724.0, "title": "How do you get over a poor interview performance?", "selftext": "I recently did a hiring manager round at a company I would have loved to work for. From the beginning, the hiring manager seemed a bit disinterested and it felt like he was chatting with someone else during the interview. At one point I even saw him smiling while I was talking, and I was not saying anything remotely amusing.\n\nThat really threw me off and I got distracted, which led to me not answering some questions as well as I should have. The questions were about my past experience, things I definitely knew, and I think that ultimately contributed to my rejection.\n\nI was really looking forward to interviewing there, and in hindsight I feel like I could have done much better, especially if I had prepared a bit more. Hindsight is always 20 20. How do you get over interviews like this?", "full_text": "How do you get over a poor interview performance?\n\nI recently did a hiring manager round at a company I would have loved to work for. From the beginning, the hiring manager seemed a bit disinterested and it felt like he was chatting with someone else during the interview. At one point I even saw him smiling while I was talking, and I was not saying anything remotely amusing.\n\nThat really threw me off and I got distracted, which led to me not answering some questions as well as I should have. The questions were about my past experience, things I definitely knew, and I think that ultimately contributed to my rejection.\n\nI was really looking forward to interviewing there, and in hindsight I feel like I could have done much better, especially if I had prepared a bit more. Hindsight is always 20 20. How do you get over interviews like this?", "author": "Fig_Towel_379", "permalink": "/r/datascience/comments/1qkzkgd/how_do_you_get_over_a_poor_interview_performance/", "url": "https://www.reddit.com/r/datascience/comments/1qkzkgd/how_do_you_get_over_a_poor_interview_performance/", "score": 52, "num_comments": 29, "upvote_ratio": 0.91, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qkw300", "created_utc": 1769187151.0, "title": "[D] Bayesian probability vs t-test for A/B testing", "selftext": "", "full_text": "[D] Bayesian probability vs t-test for A/B testing", "author": "SingerEast1469", "permalink": "/r/datascience/comments/1qkw300/d_bayesian_probability_vs_ttest_for_ab_testing/", "url": "/r/statistics/comments/1qkv067/d_bayesian_probability_vs_ttest_for_ab_testing/", "score": 12, "num_comments": 15, "upvote_ratio": 0.81, "over_18": false}
{"source": "reddit", "subreddit": "datascience", "collected_at": "2026-02-12T14:40:47.013975+00:00", "post_id": "1qjoqu2", "created_utc": 1769068991.0, "title": "Do you still use notebooks in DS?", "selftext": "I work as a data scientist and I usually build models in a notebook and then create them into a python script for deployment. Lately, I’ve been wondering if this is the most efficient approach and I’m curious to learn about any hacks, workflows or processes you use to speed things up or stay organized.\n\nEspecially now that AI tools are everywhere and GenAI still not great at working with notebooks.", "full_text": "Do you still use notebooks in DS?\n\nI work as a data scientist and I usually build models in a notebook and then create them into a python script for deployment. Lately, I’ve been wondering if this is the most efficient approach and I’m curious to learn about any hacks, workflows or processes you use to speed things up or stay organized.\n\nEspecially now that AI tools are everywhere and GenAI still not great at working with notebooks.", "author": "codiecutie", "permalink": "/r/datascience/comments/1qjoqu2/do_you_still_use_notebooks_in_ds/", "url": "https://www.reddit.com/r/datascience/comments/1qjoqu2/do_you_still_use_notebooks_in_ds/", "score": 92, "num_comments": 74, "upvote_ratio": 0.96, "over_18": false}
