{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2uova", "created_utc": 1770906116.0, "title": "Am I being anxious too early?", "selftext": "So, I'm a third year (6th Semester) Data Science student, doing double degrees, both in DS (stupid i know) and I've recently started applying for jobs/internships. I've had 2 proper internships in the past 4 months in total. Had me doing mostly DA stuff, and I worked one time on a prod copy PostgreSQL DB but they just had me writing SQL queries for 2 months and nothing else.\n\n  \nSo to finally take things seriously I started building a DE Project. FX Rates ETL Pipeline which is now fully dockerized and orchestrated using Airflow. Migrating it to AWS to learn how the whole shebang works. Gonna try to apply backfills and maybe add a SLM layer on top for fun. By now, I've applied to 20 companies out of which 2 have rejected me and 18 are still pending. I'm targeting startups and remote work as I still have 3 more semesters to complete and I'm aware that I'm not cracked and there's a massive skill issue but It's just seeing those job requirements messes with my head and I freeze breaking my productive and fun building streak. I do not know what to do anymore. What to build what other technologies to learn what other projects to build cuz there are a LOT of em. Any suggestions/comments are welcome. Thank you.", "full_text": "Am I being anxious too early?\n\nSo, I'm a third year (6th Semester) Data Science student, doing double degrees, both in DS (stupid i know) and I've recently started applying for jobs/internships. I've had 2 proper internships in the past 4 months in total. Had me doing mostly DA stuff, and I worked one time on a prod copy PostgreSQL DB but they just had me writing SQL queries for 2 months and nothing else.\n\n  \nSo to finally take things seriously I started building a DE Project. FX Rates ETL Pipeline which is now fully dockerized and orchestrated using Airflow. Migrating it to AWS to learn how the whole shebang works. Gonna try to apply backfills and maybe add a SLM layer on top for fun. By now, I've applied to 20 companies out of which 2 have rejected me and 18 are still pending. I'm targeting startups and remote work as I still have 3 more semesters to complete and I'm aware that I'm not cracked and there's a massive skill issue but It's just seeing those job requirements messes with my head and I freeze breaking my productive and fun building streak. I do not know what to do anymore. What to build what other technologies to learn what other projects to build cuz there are a LOT of em. Any suggestions/comments are welcome. Thank you.", "author": "XtremeSenpai", "permalink": "/r/dataengineering/comments/1r2uova/am_i_being_anxious_too_early/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2uova/am_i_being_anxious_too_early/", "score": 2, "num_comments": 1, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2uicu", "created_utc": 1770905666.0, "title": "AI For Data Modelling??", "selftext": "TL;DR\n\n***Claude code + DW MCP server = Reliable Data Models***\n\nHey guys! I have been a data engineer for 10+ years now and have worked at several big tech companies and was always skeptical at LLM's ability to reason over messy data sources to produce reliable fct/agg tables to service business analytics. My experience had been that they lack the domain knowledge for the data sources and business rules. \n\n**HOWEVER**… This week I built a data mart (dbt+duckdb) sourced from a very messy and obscure data source coming from a legacy (think 80s SW) ERP system, with claude code and was blown away by the results!!\n\nI found that giving claude code the following produced exceptional results basically in one shot! (*footer has this laid out in more details*)\n\n* A duckdb MCP server so that it can explore the raw data itself \n* **VERY** clear explanations on the analytical use cases \n* **VERY** explicit data modelling patterns (raw -&gt; stg -&gt; fct -&gt; agg)\n* Quick blurb on what I know about the source system and encouraging it to search online and learn more before diving in\n\nThe data mart produced was clean, effective, easy to query, and most importantly correct and reliable. Hierarchy was respected all agg sourced from fct all fct from stg and all stg from source. It built a few robust core fct tables that then serviced multiple aggs for each analytical use case I outlined. I was using DBT so in my prompt I stressed data quality and trust so it added tests. \n\nWith 10+ years of experience it would have probably taken me a week to build, what claude code did in an afternoon. While this data mart still would still require further testing and QA before I would be confident in rolling it out to the broader org it made me realize that AI can in fact write high quality SQL.\n\nThis experiment got me thinking... As these base models keep getting better (this was on Opus 4.6) the research, reason, explore, build, test loop that I prompted my claude code to do for this project is only going to get better. So that means 1 DE who knows what they are doing and understands core data modelling principles really well can in fact replace an entire DE team and move much faster **IF** they are able to harness the true power of these AI agents.\n\nMy next experiment is going to be trying to bundle my learnings from this project into a skill and just letting loose on a new data source and seeing what comes out.\n\nCurious has anyone else done something similar? Would also love to hear peoples thoughts on AI agents in the realm of DE where mistakes are really costly and you basically cant afford even 1 because stakeholders will loose trust instantly and never touch your data assets again.\n\n\\------------\n\n***Technical Notes***\n\n* AI Agent = Claude code/Opus 4.6\n* Source data was in a MSSQL Server\n* Relevant source tables extracted to a duckdb database in their raw form\n* Final DB was another duckdb db\n* DBT used for transformations\n* Motherduck Duckdb MCP server so the AI Agent can query the db's (although sometimes I noticed Claude just resorted to using the duckdb cli or running via python -c) \n* High level workflow; \n   * Explain to agent what produced the source data, what analytical use cases we want to service, what data modelling patterns to follow, ask it to do research and come back to me. \n   * Go back and forth clarifying a few things\n   * Ask it to use the MCP server to explore the raw data and run exploratory queries so it can get its bearings\n   * Enter plan mode and ask it to start designing the data mart, review the plan, discuss as needed, and then let it execute\n   * Ask it to use the MCP server to QA the data mart it produced (apply fixes if needed)\n   * Ask it to verify metric values sourced from data mart vs. raw data (apply fixes if needed)\n\n[DBT produced lineage graph \\(sorry for it being unreadable but this was for a client and they would like table names to remain private.... green = source tables\\)](https://preview.redd.it/gl5bvx8hn2jg1.png?width=706&amp;format=png&amp;auto=webp&amp;s=305954d6c8b0edf58138568f87c0f8e61cbf1482)\n\n", "full_text": "AI For Data Modelling??\n\nTL;DR\n\n***Claude code + DW MCP server = Reliable Data Models***\n\nHey guys! I have been a data engineer for 10+ years now and have worked at several big tech companies and was always skeptical at LLM's ability to reason over messy data sources to produce reliable fct/agg tables to service business analytics. My experience had been that they lack the domain knowledge for the data sources and business rules. \n\n**HOWEVER**… This week I built a data mart (dbt+duckdb) sourced from a very messy and obscure data source coming from a legacy (think 80s SW) ERP system, with claude code and was blown away by the results!!\n\nI found that giving claude code the following produced exceptional results basically in one shot! (*footer has this laid out in more details*)\n\n* A duckdb MCP server so that it can explore the raw data itself \n* **VERY** clear explanations on the analytical use cases \n* **VERY** explicit data modelling patterns (raw -&gt; stg -&gt; fct -&gt; agg)\n* Quick blurb on what I know about the source system and encouraging it to search online and learn more before diving in\n\nThe data mart produced was clean, effective, easy to query, and most importantly correct and reliable. Hierarchy was respected all agg sourced from fct all fct from stg and all stg from source. It built a few robust core fct tables that then serviced multiple aggs for each analytical use case I outlined. I was using DBT so in my prompt I stressed data quality and trust so it added tests. \n\nWith 10+ years of experience it would have probably taken me a week to build, what claude code did in an afternoon. While this data mart still would still require further testing and QA before I would be confident in rolling it out to the broader org it made me realize that AI can in fact write high quality SQL.\n\nThis experiment got me thinking... As these base models keep getting better (this was on Opus 4.6) the research, reason, explore, build, test loop that I prompted my claude code to do for this project is only going to get better. So that means 1 DE who knows what they are doing and understands core data modelling principles really well can in fact replace an entire DE team and move much faster **IF** they are able to harness the true power of these AI agents.\n\nMy next experiment is going to be trying to bundle my learnings from this project into a skill and just letting loose on a new data source and seeing what comes out.\n\nCurious has anyone else done something similar? Would also love to hear peoples thoughts on AI agents in the realm of DE where mistakes are really costly and you basically cant afford even 1 because stakeholders will loose trust instantly and never touch your data assets again.\n\n\\------------\n\n***Technical Notes***\n\n* AI Agent = Claude code/Opus 4.6\n* Source data was in a MSSQL Server\n* Relevant source tables extracted to a duckdb database in their raw form\n* Final DB was another duckdb db\n* DBT used for transformations\n* Motherduck Duckdb MCP server so the AI Agent can query the db's (although sometimes I noticed Claude just resorted to using the duckdb cli or running via python -c) \n* High level workflow; \n   * Explain to agent what produced the source data, what analytical use cases we want to service, what data modelling patterns to follow, ask it to do research and come back to me. \n   * Go back and forth clarifying a few things\n   * Ask it to use the MCP server to explore the raw data and run exploratory queries so it can get its bearings\n   * Enter plan mode and ask it to start designing the data mart, review the plan, discuss as needed, and then let it execute\n   * Ask it to use the MCP server to QA the data mart it produced (apply fixes if needed)\n   * Ask it to verify metric values sourced from data mart vs. raw data (apply fixes if needed)\n\n[DBT produced lineage graph \\(sorry for it being unreadable but this was for a client and they would like table names to remain private.... green = source tables\\)](https://preview.redd.it/gl5bvx8hn2jg1.png?width=706&amp;format=png&amp;auto=webp&amp;s=305954d6c8b0edf58138568f87c0f8e61cbf1482)", "author": "No_Rhubarb7903", "permalink": "/r/dataengineering/comments/1r2uicu/ai_for_data_modelling/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2uicu/ai_for_data_modelling/", "score": 1, "num_comments": 4, "upvote_ratio": 0.67, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2tpnt", "created_utc": 1770903663.0, "title": "Matching Records", "selftext": "For those working with 30–40+ customer tables across different systems without MDM or CDP budgets. How are you reconciling identities to create a reliable source of truth?\n\nAre you using formal identity resolution, survivorship rules, probabilistic matching… or handling it at the modeling layer?", "full_text": "Matching Records\n\nFor those working with 30–40+ customer tables across different systems without MDM or CDP budgets. How are you reconciling identities to create a reliable source of truth?\n\nAre you using formal identity resolution, survivorship rules, probabilistic matching… or handling it at the modeling layer?", "author": "dreyybaba", "permalink": "/r/dataengineering/comments/1r2tpnt/matching_records/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2tpnt/matching_records/", "score": 2, "num_comments": 1, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2svde", "created_utc": 1770901424.0, "title": "What to learn besides DE", "selftext": "I come from a non-engineering background and I'll be facing my first DE role soon (coming from pura anlytics and stats). I want to move towards a more infra role in the future (3 years), something more aligned to IT rather than business. Apart from what I would be using in my day day work (python, sql, dbt, yaml, data modelling) what would you recommend to learn, read and practice in study times to advance towards infra cloud services? Books, blogs, certs, anything is welcomed. Thanks", "full_text": "What to learn besides DE\n\nI come from a non-engineering background and I'll be facing my first DE role soon (coming from pura anlytics and stats). I want to move towards a more infra role in the future (3 years), something more aligned to IT rather than business. Apart from what I would be using in my day day work (python, sql, dbt, yaml, data modelling) what would you recommend to learn, read and practice in study times to advance towards infra cloud services? Books, blogs, certs, anything is welcomed. Thanks", "author": "Icy-Ask-6070", "permalink": "/r/dataengineering/comments/1r2svde/what_to_learn_besides_de/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2svde/what_to_learn_besides_de/", "score": 3, "num_comments": 2, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2shmx", "created_utc": 1770900367.0, "title": "I want to practise Dimensional Data Modelling but im lost", "selftext": "For context im in my second year in college and i want to build 3 projects to start applying for internships. \n\nFirst project i planned was building a series of ETL pipelines that would make up the ingestion and transformation layer, which would later load into my SQL database, modelled in dimensional data modelling.\n\nBut i am unable to find a suitable api or csv to get data that i can break down into a dimensional data model. I am lost.\n\nso, kindly help me solve this problem. Also leave any other project idea you might have that would help me gain experience .", "full_text": "I want to practise Dimensional Data Modelling but im lost\n\nFor context im in my second year in college and i want to build 3 projects to start applying for internships. \n\nFirst project i planned was building a series of ETL pipelines that would make up the ingestion and transformation layer, which would later load into my SQL database, modelled in dimensional data modelling.\n\nBut i am unable to find a suitable api or csv to get data that i can break down into a dimensional data model. I am lost.\n\nso, kindly help me solve this problem. Also leave any other project idea you might have that would help me gain experience .", "author": "dumb_user_404", "permalink": "/r/dataengineering/comments/1r2shmx/i_want_to_practise_dimensional_data_modelling_but/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2shmx/i_want_to_practise_dimensional_data_modelling_but/", "score": 2, "num_comments": 1, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2q47n", "created_utc": 1770892594.0, "title": "11 Compaction Strategies for Iceberg Data Lakes", "selftext": "", "full_text": "11 Compaction Strategies for Iceberg Data Lakes", "author": "codingdecently", "permalink": "/r/dataengineering/comments/1r2q47n/11_compaction_strategies_for_iceberg_data_lakes/", "url": "https://overcast.blog/11-compaction-strategies-for-iceberg-data-lakes-906d347af0f9", "score": 2, "num_comments": 2, "upvote_ratio": 0.75, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2pzwc", "created_utc": 1770892152.0, "title": "ERP sysadmin vs Data Engineering", "selftext": "Would you continue the path of being an ERP sysadmin or change career paths to data engineering? I am in between crossroads and don't know what to do. Data engineering is more mentally stimulating, but being and ERP admin is niche and gives me higher job security (maybe less earning potential in the future). Thanks", "full_text": "ERP sysadmin vs Data Engineering\n\nWould you continue the path of being an ERP sysadmin or change career paths to data engineering? I am in between crossroads and don't know what to do. Data engineering is more mentally stimulating, but being and ERP admin is niche and gives me higher job security (maybe less earning potential in the future). Thanks", "author": "Icy-Ask-6070", "permalink": "/r/dataengineering/comments/1r2pzwc/erp_sysadmin_vs_data_engineering/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2pzwc/erp_sysadmin_vs_data_engineering/", "score": 3, "num_comments": 7, "upvote_ratio": 0.81, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2pumc", "created_utc": 1770891628.0, "title": "I built a website to centralize articles, events and podcasts about data", "selftext": "I'll keep it short. I was tired of having to check a dozen different places just to keep up with the data ecosystem. It felt chaotic and I was wasting too much time.\n\nThen, I built dataaaaa! (yes, 5 a's). It started as a project to learn Cursor, but it ended up being actually useful. It’s a central hub that aggregates automatically articles, release notes, events and podcasts.\n\nWhat it does:\n\n* Feed: Tracks the data landscape so you don't have to doomscroll.\n* AI Filters: Lets you find resources by specific tech stack/topic.\n* Library: Lets you save stuff for later.\n\nI spent the last two months building this on my free time.  \nGive it a try and let me know if it's useful or what I should change!\n\n[https://www.dataaaaa.com/](https://www.dataaaaa.com/)", "full_text": "I built a website to centralize articles, events and podcasts about data\n\nI'll keep it short. I was tired of having to check a dozen different places just to keep up with the data ecosystem. It felt chaotic and I was wasting too much time.\n\nThen, I built dataaaaa! (yes, 5 a's). It started as a project to learn Cursor, but it ended up being actually useful. It’s a central hub that aggregates automatically articles, release notes, events and podcasts.\n\nWhat it does:\n\n* Feed: Tracks the data landscape so you don't have to doomscroll.\n* AI Filters: Lets you find resources by specific tech stack/topic.\n* Library: Lets you save stuff for later.\n\nI spent the last two months building this on my free time.  \nGive it a try and let me know if it's useful or what I should change!\n\n[https://www.dataaaaa.com/](https://www.dataaaaa.com/)", "author": "alphter", "permalink": "/r/dataengineering/comments/1r2pumc/i_built_a_website_to_centralize_articles_events/", "url": "https://i.redd.it/0ovtcb4jc0jg1.png", "score": 13, "num_comments": 2, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2p04z", "created_utc": 1770888448.0, "title": "How are you protecting your repos in the age of AI, especially in data engineering?", "selftext": "Look, I think whether you like AI or not, its going to find a way into your repos. Whether thats through code suggestions, agents or actual copy pasting from ChatGPT  \n  \nHow are you giving yourself the best chance of catching bugs early? Especially subtle ones in SQL, data transformations, or dbt models that \"look right\" but are logically wrong.  \n  \nOn one hand you can try help AI by adding instruction files like `CLAUDE.md` or `AGENTS.md` which they can use as added context. One the other hand you can leverage CI, precommit hooks and unit tests \n\nMy company has asked me to come up with a plan for this since some of our repos are open source, its not as simple as prohibiting AI. We don't mind people using AI but we need some guardrails to protect ourselves", "full_text": "How are you protecting your repos in the age of AI, especially in data engineering?\n\nLook, I think whether you like AI or not, its going to find a way into your repos. Whether thats through code suggestions, agents or actual copy pasting from ChatGPT  \n  \nHow are you giving yourself the best chance of catching bugs early? Especially subtle ones in SQL, data transformations, or dbt models that \"look right\" but are logically wrong.  \n  \nOn one hand you can try help AI by adding instruction files like `CLAUDE.md` or `AGENTS.md` which they can use as added context. One the other hand you can leverage CI, precommit hooks and unit tests \n\nMy company has asked me to come up with a plan for this since some of our repos are open source, its not as simple as prohibiting AI. We don't mind people using AI but we need some guardrails to protect ourselves", "author": "AverageGradientBoost", "permalink": "/r/dataengineering/comments/1r2p04z/how_are_you_protecting_your_repos_in_the_age_of/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2p04z/how_are_you_protecting_your_repos_in_the_age_of/", "score": 0, "num_comments": 11, "upvote_ratio": 0.27, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2nrv3", "created_utc": 1770883701.0, "title": "Am I cooked?", "selftext": "Will keep this as short and sweet as possible.\n\nJoined current company as an intern gave it 1000% got offered full time under the title of:\n\nJunior Data Engineer.\n\nDespite this being my title the nature of the company allowed me work with basic ETL, dash boarding, SQL and Python. I also developed some internal streamit applications for teams to input information directly into the database using a user friendly UI.\n\nWhy am I potentially cooked?\n\nData stack consists of Snowflake, Tableau and and Snaplogic (a low code drag and drop etl tool). I realised early that this low code tool would hinder me in the future so I worked on using it as a place to experiment with metadata based ingestion and create fast solutions. \n\nNow that I’ve been placed on work for a year that is 80% non DE related aka SQL copying/report bug fixing Whilst initially I’d go above and beyond to build additional pipelines and solutions I feel as though I’ve burnt out.\n\nI asked to alter this work flow to something more aligned with my role this time last year. I was told I’d finally be moving onto data product development this year April (in effect I’ve been begging to just do what I should have been doing) and I’ve realised even if I begin this work in April I’m still at almost three years experience with the same salary I was offered when I went full time and no mention or promise of an increase.\n\nI know the smart answer is to keep collecting the pay check until I can land something else but all motivation is gone. The work they have me doing is relatively easy it just doesn’t interest me whatsoever. At this rate my performance will continue to drop for lack of any incentive to continue besides collecting this current pay check.\n\nI’ve had some interviews which are offering 20-25% more than my current role, interpersonally I succeed and am able to progress but in the technical sections I struggle without resources. I’d say I’m a good problem solver but poor at syntax memorisation and coding from scratch. I tend to use examples from online along with documentation to create my solutions but a lot of interviews want off the dome anwers…\n\nHas anyone been in a similar position and what did you do to move on from it?\n\nTldr:\nAlmost at 3 years experience, level of experience technically lagging behind timeframe due to exposure at work being limited and lack of personal growth. Getting interviews but struggling with answering without resources.", "full_text": "Am I cooked?\n\nWill keep this as short and sweet as possible.\n\nJoined current company as an intern gave it 1000% got offered full time under the title of:\n\nJunior Data Engineer.\n\nDespite this being my title the nature of the company allowed me work with basic ETL, dash boarding, SQL and Python. I also developed some internal streamit applications for teams to input information directly into the database using a user friendly UI.\n\nWhy am I potentially cooked?\n\nData stack consists of Snowflake, Tableau and and Snaplogic (a low code drag and drop etl tool). I realised early that this low code tool would hinder me in the future so I worked on using it as a place to experiment with metadata based ingestion and create fast solutions. \n\nNow that I’ve been placed on work for a year that is 80% non DE related aka SQL copying/report bug fixing Whilst initially I’d go above and beyond to build additional pipelines and solutions I feel as though I’ve burnt out.\n\nI asked to alter this work flow to something more aligned with my role this time last year. I was told I’d finally be moving onto data product development this year April (in effect I’ve been begging to just do what I should have been doing) and I’ve realised even if I begin this work in April I’m still at almost three years experience with the same salary I was offered when I went full time and no mention or promise of an increase.\n\nI know the smart answer is to keep collecting the pay check until I can land something else but all motivation is gone. The work they have me doing is relatively easy it just doesn’t interest me whatsoever. At this rate my performance will continue to drop for lack of any incentive to continue besides collecting this current pay check.\n\nI’ve had some interviews which are offering 20-25% more than my current role, interpersonally I succeed and am able to progress but in the technical sections I struggle without resources. I’d say I’m a good problem solver but poor at syntax memorisation and coding from scratch. I tend to use examples from online along with documentation to create my solutions but a lot of interviews want off the dome anwers…\n\nHas anyone been in a similar position and what did you do to move on from it?\n\nTldr:\nAlmost at 3 years experience, level of experience technically lagging behind timeframe due to exposure at work being limited and lack of personal growth. Getting interviews but struggling with answering without resources.", "author": "Slik350", "permalink": "/r/dataengineering/comments/1r2nrv3/am_i_cooked/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2nrv3/am_i_cooked/", "score": 7, "num_comments": 16, "upvote_ratio": 0.6, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2j1ae", "created_utc": 1770867883.0, "title": "My brain freezes while solving or writing SQL queries.", "selftext": "I am trying so hard to get in sync with SQL, but whenever I get into any Q&amp;A with HR, my brain freezes and I forget everything. I am good at other things like communication and my other skills, but I don’t know how to fix this issue.\n\nHow do you guys actually prepare for SQL, and how can I make myself better at it?", "full_text": "My brain freezes while solving or writing SQL queries.\n\nI am trying so hard to get in sync with SQL, but whenever I get into any Q&amp;A with HR, my brain freezes and I forget everything. I am good at other things like communication and my other skills, but I don’t know how to fix this issue.\n\nHow do you guys actually prepare for SQL, and how can I make myself better at it?", "author": "Useful-Bug9391", "permalink": "/r/dataengineering/comments/1r2j1ae/my_brain_freezes_while_solving_or_writing_sql/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2j1ae/my_brain_freezes_while_solving_or_writing_sql/", "score": 7, "num_comments": 30, "upvote_ratio": 0.71, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2gu5c", "created_utc": 1770861806.0, "title": "Should I prioritize easy/medium or hard questions from DataLemur as a new graduate?", "selftext": "Hi all, I'll be graduating June so I'm currently applying to data roles with previous data engineering internships at a T100 company. I've picked up DataLemur and I'm somewhat comfortable with all easy/medium questions listed in DataLemur. Should I walk through these again to ensure I am 100% confident in answering these, or should I move onto hard questions? ", "full_text": "Should I prioritize easy/medium or hard questions from DataLemur as a new graduate?\n\nHi all, I'll be graduating June so I'm currently applying to data roles with previous data engineering internships at a T100 company. I've picked up DataLemur and I'm somewhat comfortable with all easy/medium questions listed in DataLemur. Should I walk through these again to ensure I am 100% confident in answering these, or should I move onto hard questions?", "author": "SIumped", "permalink": "/r/dataengineering/comments/1r2gu5c/should_i_prioritize_easymedium_or_hard_questions/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2gu5c/should_i_prioritize_easymedium_or_hard_questions/", "score": 32, "num_comments": 7, "upvote_ratio": 0.87, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2f8bf", "created_utc": 1770857444.0, "title": "Got tired of spinning up Flink to power a live dashboard, so I built a minimal Arrow-compatible data engine in Rust. Would love to hear your thoughts.", "selftext": "Most data engineering stacks are optimised for batch and scale. That’s fine until you actually need low-latency analytics, live dashboards, or fast iteration on streaming data - then you’re suddenly standing up Flink, renting beefy cloud instances, or duct-taping together tools that were never designed for the job. Even worse - you go to push it into Databricks that you are paying 20k a month for and it doesn’t really stream. Mate.\n\nI kept running into this, so I’ve been building [Minarrow](https://github.com/pbower/minarrow) \\- a fast, minimal columnar data library that’s wire-compatible with Apache Arrow but purpose-built to run efficiently on a single machine.\n\n**What it does:**\n\n* Core data building block paired with “*SIMD-Kernels*” crate -&gt; delivers sub-second aggregations on laptop-class hardware - no cluster, no JVM/Java OOM, no orchestrator\n* Drives live dashboards directly from streaming data without an intermediate warehouse or materialised view layer (you and/or your mate Claude still need to wire it up yourself)\n* Converts to Arrow, Polars, or PyArrow at the boundary via zero-copy, so it slots into existing ecosystems without serialisation overhead *(.to\\\\\\_polars() in Rust)*\n* Pairs with a companion crate (*Lightstream*) if you want to push results straight to the browser over WebSocket\n\n**Where it fits** *(and where it doesn’t):*\n\nThis sits at pipeline as code, or the engine-internals level. It’s a building block for engineers who are comfortable constructing pipelines and systems, not a plug-and-play BI tool. If your workload is distributed and you genuinely need horizontal scale, keep using Spark/Flink - Minarrow won’t replace that.\n\nBut if you’re in the zone - and prefer compiling for performance, and working with the blocks you need, this is the layer I wanted to exist and couldn’t find.\n\nHappy to answer questions, take criticism, or hear what you feel you’ve actually been missing in your stack.\n\nAlso, if you’ve focused more on the Python side happy to help point you into Rust land.\n\nThanks for checking it out.", "full_text": "Got tired of spinning up Flink to power a live dashboard, so I built a minimal Arrow-compatible data engine in Rust. Would love to hear your thoughts.\n\nMost data engineering stacks are optimised for batch and scale. That’s fine until you actually need low-latency analytics, live dashboards, or fast iteration on streaming data - then you’re suddenly standing up Flink, renting beefy cloud instances, or duct-taping together tools that were never designed for the job. Even worse - you go to push it into Databricks that you are paying 20k a month for and it doesn’t really stream. Mate.\n\nI kept running into this, so I’ve been building [Minarrow](https://github.com/pbower/minarrow) \\- a fast, minimal columnar data library that’s wire-compatible with Apache Arrow but purpose-built to run efficiently on a single machine.\n\n**What it does:**\n\n* Core data building block paired with “*SIMD-Kernels*” crate -&gt; delivers sub-second aggregations on laptop-class hardware - no cluster, no JVM/Java OOM, no orchestrator\n* Drives live dashboards directly from streaming data without an intermediate warehouse or materialised view layer (you and/or your mate Claude still need to wire it up yourself)\n* Converts to Arrow, Polars, or PyArrow at the boundary via zero-copy, so it slots into existing ecosystems without serialisation overhead *(.to\\\\\\_polars() in Rust)*\n* Pairs with a companion crate (*Lightstream*) if you want to push results straight to the browser over WebSocket\n\n**Where it fits** *(and where it doesn’t):*\n\nThis sits at pipeline as code, or the engine-internals level. It’s a building block for engineers who are comfortable constructing pipelines and systems, not a plug-and-play BI tool. If your workload is distributed and you genuinely need horizontal scale, keep using Spark/Flink - Minarrow won’t replace that.\n\nBut if you’re in the zone - and prefer compiling for performance, and working with the blocks you need, this is the layer I wanted to exist and couldn’t find.\n\nHappy to answer questions, take criticism, or hear what you feel you’ve actually been missing in your stack.\n\nAlso, if you’ve focused more on the Python side happy to help point you into Rust land.\n\nThanks for checking it out.", "author": "peterxsyd", "permalink": "/r/dataengineering/comments/1r2f8bf/got_tired_of_spinning_up_flink_to_power_a_live/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2f8bf/got_tired_of_spinning_up_flink_to_power_a_live/", "score": 1, "num_comments": 2, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2ann0", "created_utc": 1770846312.0, "title": "How do you push data from one api to another", "selftext": "So I'm using nextjs for context with a stack of React and Typescript. And I'm trying to basically use the JSON data push my username from github to a notion project(its nothing of value for a project I'm just trying to learn how to do it).\n\nSo how would I go about doing that, like I'd need a GET and POST request, but I've found nothing online that's useful for what I'm looking for.\n\nAnd I do have the github and notion setup and for notion I got it working, but I have to manually enter what i want to push to notion through my code or postman so its not viable at all for a real project.\n\nMy vision was to make a button with an onsubmit and then when you click it, it sends your github username to a notion project.", "full_text": "How do you push data from one api to another\n\nSo I'm using nextjs for context with a stack of React and Typescript. And I'm trying to basically use the JSON data push my username from github to a notion project(its nothing of value for a project I'm just trying to learn how to do it).\n\nSo how would I go about doing that, like I'd need a GET and POST request, but I've found nothing online that's useful for what I'm looking for.\n\nAnd I do have the github and notion setup and for notion I got it working, but I have to manually enter what i want to push to notion through my code or postman so its not viable at all for a real project.\n\nMy vision was to make a button with an onsubmit and then when you click it, it sends your github username to a notion project.", "author": "NoTap8152", "permalink": "/r/dataengineering/comments/1r2ann0/how_do_you_push_data_from_one_api_to_another/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2ann0/how_do_you_push_data_from_one_api_to_another/", "score": 3, "num_comments": 3, "upvote_ratio": 0.8, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2649o", "created_utc": 1770836188.0, "title": "What's a senior-level data engineering project that won't make me pay for cloud bs?", "selftext": "What mid-to-advanced data engineering project could I build to put on my CV that doesn't simply involve transforming a .csv into a star schema in a SQL database using pandas (junior project) but also doesn't involve me paying for Databricks/AWS/Azure or anything in the cloud because I already woke up with a 7$ bill on Databricks for processing a single JSON file multiple times while testing something.\n\nThis project should be something that can be scheduled to run periodically, not on a static dataset (an ETL pipeline that runs only once to process a dataset on Kaggle is more of a data analyst project imo) and that would have zero cost. Is it possible to build something like this or am I asking the impossible? For example, could I build a medallion-like architecture all on my local PC with data from free public APIs? If so, what tools would I use?", "full_text": "What's a senior-level data engineering project that won't make me pay for cloud bs?\n\nWhat mid-to-advanced data engineering project could I build to put on my CV that doesn't simply involve transforming a .csv into a star schema in a SQL database using pandas (junior project) but also doesn't involve me paying for Databricks/AWS/Azure or anything in the cloud because I already woke up with a 7$ bill on Databricks for processing a single JSON file multiple times while testing something.\n\nThis project should be something that can be scheduled to run periodically, not on a static dataset (an ETL pipeline that runs only once to process a dataset on Kaggle is more of a data analyst project imo) and that would have zero cost. Is it possible to build something like this or am I asking the impossible? For example, could I build a medallion-like architecture all on my local PC with data from free public APIs? If so, what tools would I use?", "author": "Lastrevio", "permalink": "/r/dataengineering/comments/1r2649o/whats_a_seniorlevel_data_engineering_project_that/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2649o/whats_a_seniorlevel_data_engineering_project_that/", "score": 0, "num_comments": 11, "upvote_ratio": 0.21, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2496c", "created_utc": 1770832220.0, "title": "Wondering what is actually the real role of a data engineer", "selftext": "Hello,\n\nSorry if this has already been asked and answered I couldn't find it.\n\n  \nI am currently learning Data Engineering through a formation. I have an intermediate level in Python to begin with but the more I move forward in the courses the more I am questioning what a Data Engineer really is. Lately I had to work on a project which took me a good 6 or 7h and the coding part was honestly quite simple but the architecture part was what took me a while.\n\n  \nAs a Data Engineer, do we expect from us to be good devs or do we expect people that know which tech stack would be the most appropriate for the use case. Even if they don't necessarily know how to use it yet?", "full_text": "Wondering what is actually the real role of a data engineer\n\nHello,\n\nSorry if this has already been asked and answered I couldn't find it.\n\n  \nI am currently learning Data Engineering through a formation. I have an intermediate level in Python to begin with but the more I move forward in the courses the more I am questioning what a Data Engineer really is. Lately I had to work on a project which took me a good 6 or 7h and the coding part was honestly quite simple but the architecture part was what took me a while.\n\n  \nAs a Data Engineer, do we expect from us to be good devs or do we expect people that know which tech stack would be the most appropriate for the use case. Even if they don't necessarily know how to use it yet?", "author": "Theclems55", "permalink": "/r/dataengineering/comments/1r2496c/wondering_what_is_actually_the_real_role_of_a/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2496c/wondering_what_is_actually_the_real_role_of_a/", "score": 6, "num_comments": 31, "upvote_ratio": 0.6, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r22gxq", "created_utc": 1770828409.0, "title": "Data engineering but how to handle value that are clearly wrong from initial raw data", "selftext": "Good Afternoon, \n\nCurrently I'm doing a project for my own hobby using NYC trip yellow taxi records.\n\nThe idea is to use both batch (historic data) and streaming data (where I make up realistic synthetic data for the rest of the dates)\n\nI'm currently using a mediallion architecture, have completed both the bronze and silver layers. Now once doing the gold layer, I have been noticing some corrupt data.\n\nThere is a total of 1.5 million records, from the same vendor (Curb Mobility, LLC) which has a negative total amount which can only be described as falsely recorded data by the vendor.\n\n  \nI'm trying to make this more of a production ready project, so what I have done is for each record, I have added a flag \"is total amount negative\" into the silver layer. The idea is for data analyst that work on this layer to later question the vendor ect. \n\nIn regard to the gold layer, I have made another table called gold\\_data\\_quality where I put these anomalies with the number of bad records and a comment about why. \n\nIs that a good way to handle this or is there a different way people in the industry handles this type of corrupted data ?\n\n", "full_text": "Data engineering but how to handle value that are clearly wrong from initial raw data\n\nGood Afternoon, \n\nCurrently I'm doing a project for my own hobby using NYC trip yellow taxi records.\n\nThe idea is to use both batch (historic data) and streaming data (where I make up realistic synthetic data for the rest of the dates)\n\nI'm currently using a mediallion architecture, have completed both the bronze and silver layers. Now once doing the gold layer, I have been noticing some corrupt data.\n\nThere is a total of 1.5 million records, from the same vendor (Curb Mobility, LLC) which has a negative total amount which can only be described as falsely recorded data by the vendor.\n\n  \nI'm trying to make this more of a production ready project, so what I have done is for each record, I have added a flag \"is total amount negative\" into the silver layer. The idea is for data analyst that work on this layer to later question the vendor ect. \n\nIn regard to the gold layer, I have made another table called gold\\_data\\_quality where I put these anomalies with the number of bad records and a comment about why. \n\nIs that a good way to handle this or is there a different way people in the industry handles this type of corrupted data ?", "author": "Weary-Ad-817", "permalink": "/r/dataengineering/comments/1r22gxq/data_engineering_but_how_to_handle_value_that_are/", "url": "https://www.reddit.com/r/dataengineering/comments/1r22gxq/data_engineering_but_how_to_handle_value_that_are/", "score": 11, "num_comments": 19, "upvote_ratio": 0.87, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r2298y", "created_utc": 1770827944.0, "title": "Any alternative to MinIO yet?", "selftext": "A few months ago, Minio was moved to \"maintenance mode\" and is no longer being actively developed. Have you found a good open-source alternative (ideally MIT or Apache 2.0)?", "full_text": "Any alternative to MinIO yet?\n\nA few months ago, Minio was moved to \"maintenance mode\" and is no longer being actively developed. Have you found a good open-source alternative (ideally MIT or Apache 2.0)?", "author": "on_the_mark_data", "permalink": "/r/dataengineering/comments/1r2298y/any_alternative_to_minio_yet/", "url": "https://www.reddit.com/r/dataengineering/comments/1r2298y/any_alternative_to_minio_yet/", "score": 7, "num_comments": 2, "upvote_ratio": 0.9, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r1z5ow", "created_utc": 1770820896.0, "title": "Anyone done quant DE recruiting?", "selftext": "Hey guys,\n\nI’m currently positioned for and have attempted DE interviews for global macro, systematic, and low latency hedge fund loops.\n\nUnlike SWE formats where there is often a defined template (check problem type and alternates, complexity, verification, and coding style), DE loops have been very open ended.\n\nI been treating them like systems design questions, I.E. we have xyz datasets, abc use-cases, and efg upstream sources and these are the things to think about. \n\nHowever, there doesn’t seem to be a clear way on the interviewee side to make sure everything is properly enumerated etc. I know this will probably be flagged as a recruiting question, but haven’t seen much on this sub around fund data needs and problems (I.E. are silos even a thing and what are the high value problems etc) or even how to think about these problems.\n\nLet me know if anyone has attempted similar loops or if there’s a good delivery structure here, esp when engaging with managers and PMs! ", "full_text": "Anyone done quant DE recruiting?\n\nHey guys,\n\nI’m currently positioned for and have attempted DE interviews for global macro, systematic, and low latency hedge fund loops.\n\nUnlike SWE formats where there is often a defined template (check problem type and alternates, complexity, verification, and coding style), DE loops have been very open ended.\n\nI been treating them like systems design questions, I.E. we have xyz datasets, abc use-cases, and efg upstream sources and these are the things to think about. \n\nHowever, there doesn’t seem to be a clear way on the interviewee side to make sure everything is properly enumerated etc. I know this will probably be flagged as a recruiting question, but haven’t seen much on this sub around fund data needs and problems (I.E. are silos even a thing and what are the high value problems etc) or even how to think about these problems.\n\nLet me know if anyone has attempted similar loops or if there’s a good delivery structure here, esp when engaging with managers and PMs!", "author": "blenderman73", "permalink": "/r/dataengineering/comments/1r1z5ow/anyone_done_quant_de_recruiting/", "url": "https://www.reddit.com/r/dataengineering/comments/1r1z5ow/anyone_done_quant_de_recruiting/", "score": 1, "num_comments": 1, "upvote_ratio": 0.6, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r1yl3v", "created_utc": 1770819524.0, "title": "Hired as a data engineer in a startup but being used only for building analytics dashboards, how do i pivot", "selftext": "Am a solo Data Engineer at a startup. I was hired to build infrastructure and pipelines, but leadership doesn't value anything they can't \"see.\"\n\nI spend 100% of my time churning out ad-hoc dashboards that get used once and forgotten. Meanwhile, the AI team is getting all the praise and attention, even though my work supports them. Also, i think they can now build rdbms in such a way that DE work would not be required in sometime\n\nRight now, I feel like a glorified Excel support desk. How do I convince leadership to let me actually do Engineering work, or is this a lost cause and look for switch?", "full_text": "Hired as a data engineer in a startup but being used only for building analytics dashboards, how do i pivot\n\nAm a solo Data Engineer at a startup. I was hired to build infrastructure and pipelines, but leadership doesn't value anything they can't \"see.\"\n\nI spend 100% of my time churning out ad-hoc dashboards that get used once and forgotten. Meanwhile, the AI team is getting all the praise and attention, even though my work supports them. Also, i think they can now build rdbms in such a way that DE work would not be required in sometime\n\nRight now, I feel like a glorified Excel support desk. How do I convince leadership to let me actually do Engineering work, or is this a lost cause and look for switch?", "author": "aks-786", "permalink": "/r/dataengineering/comments/1r1yl3v/hired_as_a_data_engineer_in_a_startup_but_being/", "url": "https://www.reddit.com/r/dataengineering/comments/1r1yl3v/hired_as_a_data_engineer_in_a_startup_but_being/", "score": 64, "num_comments": 31, "upvote_ratio": 0.91, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r1xnlf", "created_utc": 1770817217.0, "title": "The hard part of building an AI analytics assistant wasn’t the AI", "selftext": "", "full_text": "The hard part of building an AI analytics assistant wasn’t the AI", "author": "Sicarul", "permalink": "/r/dataengineering/comments/1r1xnlf/the_hard_part_of_building_an_ai_analytics/", "url": "https://www.pulumi.com/blog/how-we-built-platybot-an-ai-powered-analytics-assistant/", "score": 0, "num_comments": 2, "upvote_ratio": 0.42, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r1u9q8", "created_utc": 1770807168.0, "title": "Useful first Data Engineering project?", "selftext": "Hi,\n\nI’m studying Informatics (5th semester) in Germany and want to move toward Data Engineering. I’m planning my first larger project and would appreciate a brief assessment.\n\nIdea: Build a small Sales / E-Commerce Data Pipeline\n\nUse a more realistic historical dataset (e.g., E-Commerce/Sales CSV)\n\n* Regular updates via an API or simulated ingestion\n* Orchestration with Airflow\n* Docker as the environment\n* PostgreSQL as the data warehouse\n* Classic DW model (facts &amp; dimensions + data mart)\n* Optional later: Feature table for a small ML experiment\n\nThe main goal is to learn clean pipeline structures, orchestration, and data warehouse modeling.\n\nFrom your perspective, would this be a reasonable entry-level project for Data Engineering?  \nIf someone has experience, especially from Germany: More generally, how is the job market? Is Data Engineering still a sought-after profession?\n\nThanks 🙂", "full_text": "Useful first Data Engineering project?\n\nHi,\n\nI’m studying Informatics (5th semester) in Germany and want to move toward Data Engineering. I’m planning my first larger project and would appreciate a brief assessment.\n\nIdea: Build a small Sales / E-Commerce Data Pipeline\n\nUse a more realistic historical dataset (e.g., E-Commerce/Sales CSV)\n\n* Regular updates via an API or simulated ingestion\n* Orchestration with Airflow\n* Docker as the environment\n* PostgreSQL as the data warehouse\n* Classic DW model (facts &amp; dimensions + data mart)\n* Optional later: Feature table for a small ML experiment\n\nThe main goal is to learn clean pipeline structures, orchestration, and data warehouse modeling.\n\nFrom your perspective, would this be a reasonable entry-level project for Data Engineering?  \nIf someone has experience, especially from Germany: More generally, how is the job market? Is Data Engineering still a sought-after profession?\n\nThanks 🙂", "author": "Psychological_Log299", "permalink": "/r/dataengineering/comments/1r1u9q8/useful_first_data_engineering_project/", "url": "https://www.reddit.com/r/dataengineering/comments/1r1u9q8/useful_first_data_engineering_project/", "score": 23, "num_comments": 13, "upvote_ratio": 0.86, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r1tcjp", "created_utc": 1770803932.0, "title": "It's nine years since 'The Rise of the Data Engineer'…what's changed?", "selftext": "See title\n\nMax Beauchemin published [The Rise of the Data Engineer](https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603) in Jan 2017 (_and [The Downfall of the Data Engineer](https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b) seven months later_).\n\nWhat's the biggest change you've seen in the industry in that time? What's stayed the same?", "full_text": "It's nine years since 'The Rise of the Data Engineer'…what's changed?\n\nSee title\n\nMax Beauchemin published [The Rise of the Data Engineer](https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603) in Jan 2017 (_and [The Downfall of the Data Engineer](https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b) seven months later_).\n\nWhat's the biggest change you've seen in the industry in that time? What's stayed the same?", "author": "rmoff", "permalink": "/r/dataengineering/comments/1r1tcjp/its_nine_years_since_the_rise_of_the_data/", "url": "https://www.reddit.com/r/dataengineering/comments/1r1tcjp/its_nine_years_since_the_rise_of_the_data/", "score": 138, "num_comments": 33, "upvote_ratio": 0.95, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r1l2ky", "created_utc": 1770776997.0, "title": "Transition time: Databricks, Snowflake, Fabric", "selftext": "Our company (US, defense contractor) is planning to transition to a modern platform from current Azure Synapse environment. Majority (\\~95%) of the data pipelines are for a lakehouse environment, so lakehouse is a key decision point. We did a poc with Fabric, but it did not really meet our need, on the following points:\n\n\\- GovCloud. Majority of the services of Fabric are still not in GCC, so commercial was the choice of poc for us. But the transition of couple of lakehouses from Synapse to the Fabric was really painful. Also, the pricing model is very ambiguous. For example, if we need powerbi premium licenses, how Fabric handles that? \n\n\\- Lakehouse Explorer does not supportfor OneLake security RW permissions. RBAC also not mature for row level security.\n\n\\- Capacity based model lead to vety unpredictable costing, and Microsoft reps were unable to provide good answers,\n\nSo we are looking to Databricks, and Snowflake. I am very curious to know thought and experiences for you'll for these platforms. To my limited toe-dipping Databricks environments, it is very well suited for lakehouse. Snowflake, not so. Do you agree with this?\n\nHow Databricks handles govcloud situations? Do they have mature services in govcloud? How is their pricing model compared to Fabric, and Snowflake?\n\nManagement is very interested in my opinion as a data engineer, and also values whatever I will decide for the long run. We have a small team of 12 with a mix of architects and data engineers. Please share your thoughts, advices, suggestions.  ", "full_text": "Transition time: Databricks, Snowflake, Fabric\n\nOur company (US, defense contractor) is planning to transition to a modern platform from current Azure Synapse environment. Majority (\\~95%) of the data pipelines are for a lakehouse environment, so lakehouse is a key decision point. We did a poc with Fabric, but it did not really meet our need, on the following points:\n\n\\- GovCloud. Majority of the services of Fabric are still not in GCC, so commercial was the choice of poc for us. But the transition of couple of lakehouses from Synapse to the Fabric was really painful. Also, the pricing model is very ambiguous. For example, if we need powerbi premium licenses, how Fabric handles that? \n\n\\- Lakehouse Explorer does not supportfor OneLake security RW permissions. RBAC also not mature for row level security.\n\n\\- Capacity based model lead to vety unpredictable costing, and Microsoft reps were unable to provide good answers,\n\nSo we are looking to Databricks, and Snowflake. I am very curious to know thought and experiences for you'll for these platforms. To my limited toe-dipping Databricks environments, it is very well suited for lakehouse. Snowflake, not so. Do you agree with this?\n\nHow Databricks handles govcloud situations? Do they have mature services in govcloud? How is their pricing model compared to Fabric, and Snowflake?\n\nManagement is very interested in my opinion as a data engineer, and also values whatever I will decide for the long run. We have a small team of 12 with a mix of architects and data engineers. Please share your thoughts, advices, suggestions.", "author": "james2441139", "permalink": "/r/dataengineering/comments/1r1l2ky/transition_time_databricks_snowflake_fabric/", "url": "https://www.reddit.com/r/dataengineering/comments/1r1l2ky/transition_time_databricks_snowflake_fabric/", "score": 9, "num_comments": 15, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r1kiiz", "created_utc": 1770775502.0, "title": "pg_lake in snowflake &amp; docker installation help", "selftext": "Hey reddit!\n\nI’m building poc around pg\\_lake in snowflake any resources/videos on building around it &amp; docker installation required for it would be highly appreciated!!!\n\nThanking in advance! ", "full_text": "pg_lake in snowflake &amp; docker installation help\n\nHey reddit!\n\nI’m building poc around pg\\_lake in snowflake any resources/videos on building around it &amp; docker installation required for it would be highly appreciated!!!\n\nThanking in advance!", "author": "Key_Card7466", "permalink": "/r/dataengineering/comments/1r1kiiz/pg_lake_in_snowflake_docker_installation_help/", "url": "https://www.reddit.com/r/dataengineering/comments/1r1kiiz/pg_lake_in_snowflake_docker_installation_help/", "score": 4, "num_comments": 5, "upvote_ratio": 0.84, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r1b92o", "created_utc": 1770753366.0, "title": "Generate Global ID", "selftext": "Background: Financial services industry with source data from a variety of CRMs due to various acquisitions and product offerings; i.e., wealth, tax, trust, investment banking. All these CRMs generate their own unique client id. \n\nOur data is centralized in Snowflake and dbt being our transformation framework for a loose medallion layer. We use Windmill as our orchestration application. Data is sourced through APIs, FiveTran, etc. \n\nChallenge: After creating a normalized client registry model in dbt for each CRM instance the data will be stacked where a global client id can be generated and assigned across instances; Andy Doe in “Wealth” and Andrew Doe in “Tax” through probabilistic matching are determined with a high degree of certainty to be the same and assigned an identifier. \n\nWe’re early in the process and have started exploring the splink library for probabilistic matching. \n\nLooking for alternatives or some general ideas how this should be approached. ", "full_text": "Generate Global ID\n\nBackground: Financial services industry with source data from a variety of CRMs due to various acquisitions and product offerings; i.e., wealth, tax, trust, investment banking. All these CRMs generate their own unique client id. \n\nOur data is centralized in Snowflake and dbt being our transformation framework for a loose medallion layer. We use Windmill as our orchestration application. Data is sourced through APIs, FiveTran, etc. \n\nChallenge: After creating a normalized client registry model in dbt for each CRM instance the data will be stacked where a global client id can be generated and assigned across instances; Andy Doe in “Wealth” and Andrew Doe in “Tax” through probabilistic matching are determined with a high degree of certainty to be the same and assigned an identifier. \n\nWe’re early in the process and have started exploring the splink library for probabilistic matching. \n\nLooking for alternatives or some general ideas how this should be approached.", "author": "South-Ambassador2326", "permalink": "/r/dataengineering/comments/1r1b92o/generate_global_id/", "url": "https://www.reddit.com/r/dataengineering/comments/1r1b92o/generate_global_id/", "score": 4, "num_comments": 5, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r19lmd", "created_utc": 1770749818.0, "title": "Power BI X Python", "selftext": "Oi, pessoal! Tenho uma dúvida e preciso muito da ajuda de vocês.\n\nFui efetivada como cientista de dados júnior e quero me desenvolver mais em banco de dados e Python. Sei o básico (funções, variáveis etc.), mas sinto que ainda não entendo bem os conceitos e a estratégia por trás das coisas.\n\nO que mais me confunde é que muitos cursos ensinam um fluxo tipo: pegar um CSV, salvar em algum lugar, limpar, subir de novo, carregar no Python, automatizar com o Windows Task… e, sendo bem sincera, isso parece pouco prático no dia a dia real de uma empresa.\n\nAqui onde trabalho temos vários dashboards, alguns bem pesados para editar, que puxam direto do banco do TI. Usamos Oracle e MySQL. Aí fico pensando: o Python não poderia se conectar direto no banco e alimentar o BI? Porque, se for para pegar dados de um banco que eu nem tenho permissão de edição, jogar no Python e depois subir para outro banco ou planilha… isso realmente compensa?\n\nTambém fico perdida porque vejo opiniões muito diferentes: tem gente que fala que Power BI é maravilhoso, outros dizem que o certo é fazer todos os gráficos no Python e que BI é ruim… e eu sinceramente não sei por onde começar nem no que focar para evoluir.\n\nOutro ponto: temos um banco em que o pessoal do TI cadastra nomes de empresas e outras informações de formas diferentes. A gente trata isso nos dashboards, mas sempre aparece uma nova variação e temos que corrigir tudo de novo. Se levássemos esse tratamento para Python, não seria o mesmo problema? Como garantir que os dados fiquem padronizados e corretos ao longo do tempo?\n\nE ainda surgem outras dúvidas:  \nonde guardar os códigos?  \ncomo organizar os projetos?  \ncomo lidar com erros?  \nquestões de segurança?\n\nO Python é tão abrangente que acabo não sabendo em que focar primeiro.\n\nSe alguém puder compartilhar como funciona esse fluxo na prática (Python + banco + BI) e o que realmente vale a pena estudar no início, eu agradeceria muito!", "full_text": "Power BI X Python\n\nOi, pessoal! Tenho uma dúvida e preciso muito da ajuda de vocês.\n\nFui efetivada como cientista de dados júnior e quero me desenvolver mais em banco de dados e Python. Sei o básico (funções, variáveis etc.), mas sinto que ainda não entendo bem os conceitos e a estratégia por trás das coisas.\n\nO que mais me confunde é que muitos cursos ensinam um fluxo tipo: pegar um CSV, salvar em algum lugar, limpar, subir de novo, carregar no Python, automatizar com o Windows Task… e, sendo bem sincera, isso parece pouco prático no dia a dia real de uma empresa.\n\nAqui onde trabalho temos vários dashboards, alguns bem pesados para editar, que puxam direto do banco do TI. Usamos Oracle e MySQL. Aí fico pensando: o Python não poderia se conectar direto no banco e alimentar o BI? Porque, se for para pegar dados de um banco que eu nem tenho permissão de edição, jogar no Python e depois subir para outro banco ou planilha… isso realmente compensa?\n\nTambém fico perdida porque vejo opiniões muito diferentes: tem gente que fala que Power BI é maravilhoso, outros dizem que o certo é fazer todos os gráficos no Python e que BI é ruim… e eu sinceramente não sei por onde começar nem no que focar para evoluir.\n\nOutro ponto: temos um banco em que o pessoal do TI cadastra nomes de empresas e outras informações de formas diferentes. A gente trata isso nos dashboards, mas sempre aparece uma nova variação e temos que corrigir tudo de novo. Se levássemos esse tratamento para Python, não seria o mesmo problema? Como garantir que os dados fiquem padronizados e corretos ao longo do tempo?\n\nE ainda surgem outras dúvidas:  \nonde guardar os códigos?  \ncomo organizar os projetos?  \ncomo lidar com erros?  \nquestões de segurança?\n\nO Python é tão abrangente que acabo não sabendo em que focar primeiro.\n\nSe alguém puder compartilhar como funciona esse fluxo na prática (Python + banco + BI) e o que realmente vale a pena estudar no início, eu agradeceria muito!", "author": "Then-Arrival-9464", "permalink": "/r/dataengineering/comments/1r19lmd/power_bi_x_python/", "url": "https://www.reddit.com/r/dataengineering/comments/1r19lmd/power_bi_x_python/", "score": 0, "num_comments": 9, "upvote_ratio": 0.31, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r17qf4", "created_utc": 1770745878.0, "title": "Anyone else using dbt Cloud's free tier for personal projects?", "selftext": "I've been playing around with dbt Cloud's free tier for some side projects, mostly just data transformations on some personal finance data. It's pretty cool, but I'm curious if others are finding it useful for similar small-scale things or if it's overkill. What other tools are you using for simple data pipelines?", "full_text": "Anyone else using dbt Cloud's free tier for personal projects?\n\nI've been playing around with dbt Cloud's free tier for some side projects, mostly just data transformations on some personal finance data. It's pretty cool, but I'm curious if others are finding it useful for similar small-scale things or if it's overkill. What other tools are you using for simple data pipelines?", "author": "Aware-Lantern141", "permalink": "/r/dataengineering/comments/1r17qf4/anyone_else_using_dbt_clouds_free_tier_for/", "url": "https://www.reddit.com/r/dataengineering/comments/1r17qf4/anyone_else_using_dbt_clouds_free_tier_for/", "score": 11, "num_comments": 3, "upvote_ratio": 0.93, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r179y4", "created_utc": 1770744887.0, "title": "Book Recommendations for DE", "selftext": "Hi i just landed a role in DE but i’ , do u guys know any good books related to the field?", "full_text": "Book Recommendations for DE\n\nHi i just landed a role in DE but i’ , do u guys know any good books related to the field?", "author": "Ok-Confidence-3286", "permalink": "/r/dataengineering/comments/1r179y4/book_recommendations_for_de/", "url": "https://www.reddit.com/r/dataengineering/comments/1r179y4/book_recommendations_for_de/", "score": 28, "num_comments": 13, "upvote_ratio": 0.93, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r15y9b", "created_utc": 1770742036.0, "title": "Migration from informatica powercenter on-premise", "selftext": "Hi everyone 👋\n\nLooking for my org's alternatives to Informatica PowerCenter on-premise, with complex ETL, with the priority of open source and community support.\n\nIn general, I'm looking for suggestions about the tools you tried for migrating.\n\nthanks 🙏", "full_text": "Migration from informatica powercenter on-premise\n\nHi everyone 👋\n\nLooking for my org's alternatives to Informatica PowerCenter on-premise, with complex ETL, with the priority of open source and community support.\n\nIn general, I'm looking for suggestions about the tools you tried for migrating.\n\nthanks 🙏", "author": "shalomtubul", "permalink": "/r/dataengineering/comments/1r15y9b/migration_from_informatica_powercenter_onpremise/", "url": "https://www.reddit.com/r/dataengineering/comments/1r15y9b/migration_from_informatica_powercenter_onpremise/", "score": 2, "num_comments": 9, "upvote_ratio": 0.76, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r15015", "created_utc": 1770739979.0, "title": "2026 State of Data Engineering Report - 1000+ responses from data engineers", "selftext": "Here's direct link:\n\n[https://joereis.github.io/practical\\_data\\_data\\_eng\\_survey/](https://joereis.github.io/practical_data_data_eng_survey/)", "full_text": "2026 State of Data Engineering Report - 1000+ responses from data engineers\n\nHere's direct link:\n\n[https://joereis.github.io/practical\\_data\\_data\\_eng\\_survey/](https://joereis.github.io/practical_data_data_eng_survey/)", "author": "DungKhuc", "permalink": "/r/dataengineering/comments/1r15015/2026_state_of_data_engineering_report_1000/", "url": "https://www.linkedin.com/posts/josephreis_recently-i-surveyed-1101-of-you-about-the-share-7426990778536583168-fqMr/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAajovEBZaTvKT0qIqHq9ItYb5C1EMVsVSY", "score": 118, "num_comments": 8, "upvote_ratio": 0.98, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r14s59", "created_utc": 1770739498.0, "title": "dbtective: Rust-based dbt metadata 'detective' and linter", "selftext": "Hi\n\nI just released dbtective v0.2.0!🕵️\n\ndbtective is a Rust-powered 'detective' for `dbt metadata` best practices in your project, CI pipeline &amp; pre-commit. The idea is to have best practices out of the box, with the flexibility to customize to your team's specific needs. Let me know if you have any questions!\n\nCheck out a demo here:  \n\\- GitHub: [https://github.com/feliblo/dbtective](https://github.com/feliblo/dbtective)  \n\\- Docs: [https://feliblo.github.io/dbtective/](https://feliblo.github.io/dbtective/)\n\nOr try it out now:  \n`pip install dbtective`  \n`dbtective init`  \n`dbtective run`", "full_text": "dbtective: Rust-based dbt metadata 'detective' and linter\n\nHi\n\nI just released dbtective v0.2.0!🕵️\n\ndbtective is a Rust-powered 'detective' for `dbt metadata` best practices in your project, CI pipeline &amp; pre-commit. The idea is to have best practices out of the box, with the flexibility to customize to your team's specific needs. Let me know if you have any questions!\n\nCheck out a demo here:  \n\\- GitHub: [https://github.com/feliblo/dbtective](https://github.com/feliblo/dbtective)  \n\\- Docs: [https://feliblo.github.io/dbtective/](https://feliblo.github.io/dbtective/)\n\nOr try it out now:  \n`pip install dbtective`  \n`dbtective init`  \n`dbtective run`", "author": "Zer0designs", "permalink": "/r/dataengineering/comments/1r14s59/dbtective_rustbased_dbt_metadata_detective_and/", "url": "https://www.reddit.com/r/dataengineering/comments/1r14s59/dbtective_rustbased_dbt_metadata_detective_and/", "score": 24, "num_comments": 3, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r14bx9", "created_utc": 1770738519.0, "title": "[AMA] We're the Trino company, ask us anything!", "selftext": "I'm u/lestermartin, Trino DevRel @ Starburst, the Trino company, and I wanted to see if I can address any questions and/or concerns around Trino, and Trino-based solutions such as Starburst. If there's anything I can't handle, I pull in folks from the Trino community and Starburst PM, eng, support &amp; field teams to make sure we address your thoughts.\n\nI loved [https://www.reddit.com/r/dataengineering/comments/1r0ff3b/ama\\_were\\_dbt\\_labs\\_ask\\_us\\_anything/](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/ama_were_dbt_labs_ask_us_anything/) promoting an AMA discussion here in r/dataengineering which drove me to post this discussion. I'll try to figure out how to request the moderators allow a similar live Q&amp;A in the future if there is significant interest generated from this post.\n\nIn the meantime, I'm hosting an 'office hours' session on Thursday, Feb 12, where folks can use chat and/or come on-stage with full audio/video and ask anything they want in the data space; [register here](https://www.starburst.io/info/starburst-office-hours-connect-once-query-everywhere/).  I'll be leading a hands-on lab on Apache Iceberg the following Thursday, Feb 19, too -- [reg link](https://www.starburst.io/info/hands-on-with-apache-iceberg-build-evolve-operate-event-webinar-light/) if interested.\n\nOkay... I'd love to hear your success, failures, questions, comments, concerns, and plans for using Trino!!", "full_text": "[AMA] We're the Trino company, ask us anything!\n\nI'm u/lestermartin, Trino DevRel @ Starburst, the Trino company, and I wanted to see if I can address any questions and/or concerns around Trino, and Trino-based solutions such as Starburst. If there's anything I can't handle, I pull in folks from the Trino community and Starburst PM, eng, support &amp; field teams to make sure we address your thoughts.\n\nI loved [https://www.reddit.com/r/dataengineering/comments/1r0ff3b/ama\\_were\\_dbt\\_labs\\_ask\\_us\\_anything/](https://www.reddit.com/r/dataengineering/comments/1r0ff3b/ama_were_dbt_labs_ask_us_anything/) promoting an AMA discussion here in r/dataengineering which drove me to post this discussion. I'll try to figure out how to request the moderators allow a similar live Q&amp;A in the future if there is significant interest generated from this post.\n\nIn the meantime, I'm hosting an 'office hours' session on Thursday, Feb 12, where folks can use chat and/or come on-stage with full audio/video and ask anything they want in the data space; [register here](https://www.starburst.io/info/starburst-office-hours-connect-once-query-everywhere/).  I'll be leading a hands-on lab on Apache Iceberg the following Thursday, Feb 19, too -- [reg link](https://www.starburst.io/info/hands-on-with-apache-iceberg-build-evolve-operate-event-webinar-light/) if interested.\n\nOkay... I'd love to hear your success, failures, questions, comments, concerns, and plans for using Trino!!", "author": "lester-martin", "permalink": "/r/dataengineering/comments/1r14bx9/ama_were_the_trino_company_ask_us_anything/", "url": "https://www.reddit.com/r/dataengineering/comments/1r14bx9/ama_were_the_trino_company_ask_us_anything/", "score": 20, "num_comments": 19, "upvote_ratio": 0.78, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r12ckn", "created_utc": 1770734052.0, "title": "How do you justify confluent cloud costs to leadership when the bill keeps climbing?", "selftext": "Our confluent bill just hit $18k this month and my manager is freaking out. We're processing around 2 million events daily, but between cluster costs, connector fees, and moving data around we're burning through money.\n\n\n\nI tried explaining that kafka needs this setup, showed him what competitors charge, but he keeps asking why we can't use something cheaper, and honestly starting to wonder the same thing. We're paying top dollar and I still spend half my time fixing cluster issues.\n\n\n\nHow do you prove it's worth it when your boss sees the bill and goes pale, we're a series b startup so every dollar counts, what are teams using these days that won't drain your budget but also won't wake you up with alerts?", "full_text": "How do you justify confluent cloud costs to leadership when the bill keeps climbing?\n\nOur confluent bill just hit $18k this month and my manager is freaking out. We're processing around 2 million events daily, but between cluster costs, connector fees, and moving data around we're burning through money.\n\n\n\nI tried explaining that kafka needs this setup, showed him what competitors charge, but he keeps asking why we can't use something cheaper, and honestly starting to wonder the same thing. We're paying top dollar and I still spend half my time fixing cluster issues.\n\n\n\nHow do you prove it's worth it when your boss sees the bill and goes pale, we're a series b startup so every dollar counts, what are teams using these days that won't drain your budget but also won't wake you up with alerts?", "author": "Funny-Affect-8718", "permalink": "/r/dataengineering/comments/1r12ckn/how_do_you_justify_confluent_cloud_costs_to/", "url": "https://www.reddit.com/r/dataengineering/comments/1r12ckn/how_do_you_justify_confluent_cloud_costs_to/", "score": 51, "num_comments": 68, "upvote_ratio": 0.88, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0ypa5", "created_utc": 1770724357.0, "title": "Next Generation DB Ingestion at Pinterest", "selftext": "", "full_text": "Next Generation DB Ingestion at Pinterest", "author": "rmoff", "permalink": "/r/dataengineering/comments/1r0ypa5/next_generation_db_ingestion_at_pinterest/", "url": "https://medium.com/pinterest-engineering/next-generation-db-ingestion-at-pinterest-66844b7153b7", "score": 29, "num_comments": 2, "upvote_ratio": 0.98, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0xk3z", "created_utc": 1770720491.0, "title": "Need suggesitions", "selftext": "Hello Everyone...  \nI am seeking suggesitions from you people I have 7 year of experience as Desktop support engineer and IT Support Engineer currently working as a support engineer in MNC in India. I know Python scripting and Azure cloud. But I wanted to move into GCP Data engineering as I know now a days every big company adapting GCP. \n\nHere my question is I wanted to switch my role to Data Engineering I ready to learn to land on Job. Is my decesion good. Why I am thinking to take this decesion is becase of my low salary.   \nPlease share your thoughts and futer scope in Data engineering .   \nThank you", "full_text": "Need suggesitions\n\nHello Everyone...  \nI am seeking suggesitions from you people I have 7 year of experience as Desktop support engineer and IT Support Engineer currently working as a support engineer in MNC in India. I know Python scripting and Azure cloud. But I wanted to move into GCP Data engineering as I know now a days every big company adapting GCP. \n\nHere my question is I wanted to switch my role to Data Engineering I ready to learn to land on Job. Is my decesion good. Why I am thinking to take this decesion is becase of my low salary.   \nPlease share your thoughts and futer scope in Data engineering .   \nThank you", "author": "Repulsive-Shine-1490", "permalink": "/r/dataengineering/comments/1r0xk3z/need_suggesitions/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0xk3z/need_suggesitions/", "score": 8, "num_comments": 4, "upvote_ratio": 0.91, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0wwrn", "created_utc": 1770718111.0, "title": "Our company successfully built an on-prem \"Lakehouse\" with Spark on K8s, Hive, Minio. What are Day 2 data engineering challenges that we will inevitably face?", "selftext": "I'm thinking \n\n\\- schema evolution for iceberg/delta lake  \n\\- small file performance issues, compaction\n\nWhat else? \n\nAny resources and best practices for on-prem Lakehouse management?", "full_text": "Our company successfully built an on-prem \"Lakehouse\" with Spark on K8s, Hive, Minio. What are Day 2 data engineering challenges that we will inevitably face?\n\nI'm thinking \n\n\\- schema evolution for iceberg/delta lake  \n\\- small file performance issues, compaction\n\nWhat else? \n\nAny resources and best practices for on-prem Lakehouse management?", "author": "seaborn_as_sns", "permalink": "/r/dataengineering/comments/1r0wwrn/our_company_successfully_built_an_onprem/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0wwrn/our_company_successfully_built_an_onprem/", "score": 44, "num_comments": 47, "upvote_ratio": 0.92, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0wk35", "created_utc": 1770716795.0, "title": "switch or stay from data scientist to mobile network engineer(data engineer)", "selftext": "I work in the uk and got and offer from a telecom company currently i work for a small mid size family business as a data scientist the salary is around 31k. The work is around recommendation system.  now i am learning stuff but got this position as a data engineer working with gcp and sql and python the salary a lot higher close to 45k - i am not sure I can stay and learn but then salary is low and in the bigger company the salary is bigger and chance to grow and move is a lot higher. \n\nAlso i worked as a data scientist in a different company worked there for 4 + years and then got this job but salary was similar   \nHas anybody been in this situation ?", "full_text": "switch or stay from data scientist to mobile network engineer(data engineer)\n\nI work in the uk and got and offer from a telecom company currently i work for a small mid size family business as a data scientist the salary is around 31k. The work is around recommendation system.  now i am learning stuff but got this position as a data engineer working with gcp and sql and python the salary a lot higher close to 45k - i am not sure I can stay and learn but then salary is low and in the bigger company the salary is bigger and chance to grow and move is a lot higher. \n\nAlso i worked as a data scientist in a different company worked there for 4 + years and then got this job but salary was similar   \nHas anybody been in this situation ?", "author": "Possible_Physics8583", "permalink": "/r/dataengineering/comments/1r0wk35/switch_or_stay_from_data_scientist_to_mobile/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0wk35/switch_or_stay_from_data_scientist_to_mobile/", "score": 3, "num_comments": 5, "upvote_ratio": 0.62, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0vjgf", "created_utc": 1770712912.0, "title": "Postgres SQL parser in Go no cgo or ai", "selftext": "Postgres SQL parser in Go. Sharing in case it’s useful.\n\nNo AI stuff, no wrappers, no runtime tricks. Just parses SQL and gives you the structure (tables, joins, filters, CTEs, etc) without running the query.\n\nWe made it because we needed something that works with CGO off (Alpine, Lambda, ARM, scratch images) and still lets us inspect query structure for tooling / analysis.\n\nour DevOps and data engineer designed the MVP, it meant to be stupid easy to use\n\nFeel free to use it, contribute open requests, whatever needed", "full_text": "Postgres SQL parser in Go no cgo or ai\n\nPostgres SQL parser in Go. Sharing in case it’s useful.\n\nNo AI stuff, no wrappers, no runtime tricks. Just parses SQL and gives you the structure (tables, joins, filters, CTEs, etc) without running the query.\n\nWe made it because we needed something that works with CGO off (Alpine, Lambda, ARM, scratch images) and still lets us inspect query structure for tooling / analysis.\n\nour DevOps and data engineer designed the MVP, it meant to be stupid easy to use\n\nFeel free to use it, contribute open requests, whatever needed", "author": "Eitamr", "permalink": "/r/dataengineering/comments/1r0vjgf/postgres_sql_parser_in_go_no_cgo_or_ai/", "url": "https://github.com/ValkDB/postgresparser", "score": 3, "num_comments": 0, "upvote_ratio": 0.67, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0uteb", "created_utc": 1770710120.0, "title": "Is it very difficult to switch between cloud providers for Data Engineers?", "selftext": "I am currently working as an Azure Data Engineer (ADF and Databricks) for past 4.5 years, and currently looking for job change. \n\nHowever, most of the openings I see are for AWS. I am atill applying to them, keeping in mind that there's a 90% chance of being rejected during screening itself. It's not like there aren't any Azure openings, but majority of the product based company DE openings are for AWS, as I saw.\n\nJust wanted to understand what's the general take is on this? Is it difficult to switch between cloud providers? Should I create a separate cv for aws and use it to apply for aws jobs, even when I know nothing about them and figure out the questions gradually? ", "full_text": "Is it very difficult to switch between cloud providers for Data Engineers?\n\nI am currently working as an Azure Data Engineer (ADF and Databricks) for past 4.5 years, and currently looking for job change. \n\nHowever, most of the openings I see are for AWS. I am atill applying to them, keeping in mind that there's a 90% chance of being rejected during screening itself. It's not like there aren't any Azure openings, but majority of the product based company DE openings are for AWS, as I saw.\n\nJust wanted to understand what's the general take is on this? Is it difficult to switch between cloud providers? Should I create a separate cv for aws and use it to apply for aws jobs, even when I know nothing about them and figure out the questions gradually?", "author": "Comfortable-Bar-9983", "permalink": "/r/dataengineering/comments/1r0uteb/is_it_very_difficult_to_switch_between_cloud/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0uteb/is_it_very_difficult_to_switch_between_cloud/", "score": 17, "num_comments": 11, "upvote_ratio": 0.95, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0t96a", "created_utc": 1770704483.0, "title": "Would you expect to perform database administration as part of a DE role?", "selftext": "We are a data team that does DE and DA. We patch SQL Server, index, query optimize etc. We are migrating to PostgreSQL and converting to sharding.\n\nHowever we also do real time streaming to ClickHouse and internal reporting thru views (BI all is self service, we just build stable metrics into views and the more complex reports as views).\n\nRight now the team isn't big enough to hire Data Engineer specific roles and Database Engineer or Data Platform Engineer specific roles but that will happen in the next year or so.\n\nRight now though we need to hire a senior that could deploy an index or respond in a DR event and restore the DB or resolve corruption if that did occur, but when none of that is going on work on building the pipleine for our postgresql migration, building out views etc. Would this scare of most Data Engineers?", "full_text": "Would you expect to perform database administration as part of a DE role?\n\nWe are a data team that does DE and DA. We patch SQL Server, index, query optimize etc. We are migrating to PostgreSQL and converting to sharding.\n\nHowever we also do real time streaming to ClickHouse and internal reporting thru views (BI all is self service, we just build stable metrics into views and the more complex reports as views).\n\nRight now the team isn't big enough to hire Data Engineer specific roles and Database Engineer or Data Platform Engineer specific roles but that will happen in the next year or so.\n\nRight now though we need to hire a senior that could deploy an index or respond in a DR event and restore the DB or resolve corruption if that did occur, but when none of that is going on work on building the pipleine for our postgresql migration, building out views etc. Would this scare of most Data Engineers?", "author": "InnerReduceJoin", "permalink": "/r/dataengineering/comments/1r0t96a/would_you_expect_to_perform_database/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0t96a/would_you_expect_to_perform_database/", "score": 5, "num_comments": 5, "upvote_ratio": 0.79, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0t0fh", "created_utc": 1770703664.0, "title": "Transition to Distributed Systems", "selftext": "Has anyone made to switch to a more infra level based type of software engineering ?What was your strategy and what prompted you to do so ? ", "full_text": "Transition to Distributed Systems\n\nHas anyone made to switch to a more infra level based type of software engineering ?What was your strategy and what prompted you to do so ?", "author": "Proud-Mammoth-2839", "permalink": "/r/dataengineering/comments/1r0t0fh/transition_to_distributed_systems/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0t0fh/transition_to_distributed_systems/", "score": 11, "num_comments": 7, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0rt35", "created_utc": 1770699837.0, "title": "Are people actually use AI in data ingestions? Looking for practical ideas", "selftext": "Hi All,  \n\n\nI have a degree in Data Science and am working as a Data Engineer (Azure Databricks)  \n\n\nI was wondering if there are any practical use cases for me to implement AI in my day to day tasks. My degree taught us mostly ML, since it was a few years ago. I am new to AI and was wondering how I should go about this? Happy to answer any questions that'll help you guys guide me better. \n\nThank you redditors :)", "full_text": "Are people actually use AI in data ingestions? Looking for practical ideas\n\nHi All,  \n\n\nI have a degree in Data Science and am working as a Data Engineer (Azure Databricks)  \n\n\nI was wondering if there are any practical use cases for me to implement AI in my day to day tasks. My degree taught us mostly ML, since it was a few years ago. I am new to AI and was wondering how I should go about this? Happy to answer any questions that'll help you guys guide me better. \n\nThank you redditors :)", "author": "[deleted]", "permalink": "/r/dataengineering/comments/1r0rt35/are_people_actually_use_ai_in_data_ingestions/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0rt35/are_people_actually_use_ai_in_data_ingestions/", "score": 57, "num_comments": 26, "upvote_ratio": 0.9, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0pico", "created_utc": 1770693246.0, "title": "Intro to Floecat: a catalog for query engines that care about cost-based optimisation", "selftext": "Hi all, we’ve just open sourced Floecat: https://github.com/eng-floe/floecat\n\nFloecat is a catalog-of-catalogs that federates Iceberg and Delta catalogs and augments them with planner-grade metadata and statistics (histograms, MCVs, PK/FK relationships, etc.) to support cost-based SQL query planning.\n\nIt exposes an Iceberg REST Catalog API, so engines like Trino and DuckDB can use it as a single canonical catalog in front of multiple upstream Iceberg catalogs.\n\nWe built Floecat because existing lakehouse catalogs focus on metadata mutation, not metadata consumption. For our own SQL engine (Floe), we needed stable, reusable statistics and relational metadata to support predictable planning over Iceberg and Delta. Floe will be available later this year, but Floecat is designed to be engine-agnostic.\n\nIf this sounds interesting, I wrote more about the motivation and design here: https://floedb.ai/blog/introducing-floecat-a-catalog-of-catalogs-for-the-modern-lakehouse\n\nFeedback is very welcome, especially from folks who’ve struggled with planning, stats, or metadata across multiple lakehouse catalogs.\n\nFull disclosure, I'm the CTO at Floe.", "full_text": "Intro to Floecat: a catalog for query engines that care about cost-based optimisation\n\nHi all, we’ve just open sourced Floecat: https://github.com/eng-floe/floecat\n\nFloecat is a catalog-of-catalogs that federates Iceberg and Delta catalogs and augments them with planner-grade metadata and statistics (histograms, MCVs, PK/FK relationships, etc.) to support cost-based SQL query planning.\n\nIt exposes an Iceberg REST Catalog API, so engines like Trino and DuckDB can use it as a single canonical catalog in front of multiple upstream Iceberg catalogs.\n\nWe built Floecat because existing lakehouse catalogs focus on metadata mutation, not metadata consumption. For our own SQL engine (Floe), we needed stable, reusable statistics and relational metadata to support predictable planning over Iceberg and Delta. Floe will be available later this year, but Floecat is designed to be engine-agnostic.\n\nIf this sounds interesting, I wrote more about the motivation and design here: https://floedb.ai/blog/introducing-floecat-a-catalog-of-catalogs-for-the-modern-lakehouse\n\nFeedback is very welcome, especially from folks who’ve struggled with planning, stats, or metadata across multiple lakehouse catalogs.\n\nFull disclosure, I'm the CTO at Floe.", "author": "farmf00d", "permalink": "/r/dataengineering/comments/1r0pico/intro_to_floecat_a_catalog_for_query_engines_that/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0pico/intro_to_floecat_a_catalog_for_query_engines_that/", "score": 4, "num_comments": 0, "upvote_ratio": 0.84, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0ljxk", "created_utc": 1770682792.0, "title": "Production Access", "selftext": "Hi. Question about production access. Does your organization allow users/developers who are not admins or in IT access to run their pipelines in production? Meaning they developed it but maybe IT provided the platform such as Airflow, nifi, etc. To run it.  If they can’t run it do they have production access but just more restricted? Like read access so that they can debug why a pipeline failed and push changes without have to ask someone to send them the logs for them to see what happened.\n\nI’m asking this since right now I’m in an org where there are a few platforms but the two biggest don’t allow anyone outside their 2-5 person teams access to it. Essentially developers are expected to build pipelines and hand them off and that’s it. No view into prod anything.   The reasoning by those admins is that developers don’t need to see prod and it’s keeps their environment secure. They will monitor and notify us if something goes wrong.  I think this is dumb honestly as in my opinion that if you can’t grant people production access and keep it secure at the same time your environment is not as good as you think.  I also think that developers need prod access if they are an engineer. At minimum I think they should have read access so that they can easy see how their pipelines are performing and debug if needed.  The environments and nifi and ssis for the record and this isn’t a post to bash them so I’m only saying that for context. I don’t care what the platform is per se but just the workflow in general.\n\nHow does your organization work? Am I missing a reason why developers should not have prod access to if they are required to build and debug pipelines?", "full_text": "Production Access\n\nHi. Question about production access. Does your organization allow users/developers who are not admins or in IT access to run their pipelines in production? Meaning they developed it but maybe IT provided the platform such as Airflow, nifi, etc. To run it.  If they can’t run it do they have production access but just more restricted? Like read access so that they can debug why a pipeline failed and push changes without have to ask someone to send them the logs for them to see what happened.\n\nI’m asking this since right now I’m in an org where there are a few platforms but the two biggest don’t allow anyone outside their 2-5 person teams access to it. Essentially developers are expected to build pipelines and hand them off and that’s it. No view into prod anything.   The reasoning by those admins is that developers don’t need to see prod and it’s keeps their environment secure. They will monitor and notify us if something goes wrong.  I think this is dumb honestly as in my opinion that if you can’t grant people production access and keep it secure at the same time your environment is not as good as you think.  I also think that developers need prod access if they are an engineer. At minimum I think they should have read access so that they can easy see how their pipelines are performing and debug if needed.  The environments and nifi and ssis for the record and this isn’t a post to bash them so I’m only saying that for context. I don’t care what the platform is per se but just the workflow in general.\n\nHow does your organization work? Am I missing a reason why developers should not have prod access to if they are required to build and debug pipelines?", "author": "fordatechy", "permalink": "/r/dataengineering/comments/1r0ljxk/production_access/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0ljxk/production_access/", "score": 0, "num_comments": 15, "upvote_ratio": 0.5, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0jbdn", "created_utc": 1770677327.0, "title": "How common is good maintenance?", "selftext": "I've noticed a company culture of prioritising features from the top down. If it's not connected to executive strategy, then it's a pet project and we should not be working on it. \n\nExecutives focus on growth that translates to new features in data engineering, so new pipelines, new AI integrations, etc. However bottom-up concerns are largely ignored, such as around lack of outage reporting, insufficient integration and unit testing, messy documentation, very inconsistent standards, insufficient metadata and data governance standards, etc. \n\nThis feels different to the perception I've had of some of the fancier workplaces, where I thought some of the best ideas and innovation came from bottom-up experimentation from the people actually on the tools.", "full_text": "How common is good maintenance?\n\nI've noticed a company culture of prioritising features from the top down. If it's not connected to executive strategy, then it's a pet project and we should not be working on it. \n\nExecutives focus on growth that translates to new features in data engineering, so new pipelines, new AI integrations, etc. However bottom-up concerns are largely ignored, such as around lack of outage reporting, insufficient integration and unit testing, messy documentation, very inconsistent standards, insufficient metadata and data governance standards, etc. \n\nThis feels different to the perception I've had of some of the fancier workplaces, where I thought some of the best ideas and innovation came from bottom-up experimentation from the people actually on the tools.", "author": "PossibilityRegular21", "permalink": "/r/dataengineering/comments/1r0jbdn/how_common_is_good_maintenance/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0jbdn/how_common_is_good_maintenance/", "score": 5, "num_comments": 3, "upvote_ratio": 0.86, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0ix99", "created_utc": 1770676420.0, "title": "awesome new extension to query Snowflake tables directly within DuckDB", "selftext": "Very cool to be able to use DuckDB's extension ecosystem with my Snowflake data now", "full_text": "awesome new extension to query Snowflake tables directly within DuckDB\n\nVery cool to be able to use DuckDB's extension ecosystem with my Snowflake data now", "author": "hornyforsavings", "permalink": "/r/dataengineering/comments/1r0ix99/awesome_new_extension_to_query_snowflake_tables/", "url": "https://blog.greybeam.ai/querying-snowflake-with-duckdb/", "score": 6, "num_comments": 2, "upvote_ratio": 0.76, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0ht6o", "created_utc": 1770673903.0, "title": "Design choice question: should distributed gateway nodes access datastore directly or only through an internal API?", "selftext": "Context:  \nI’m building a horizontally scaled proxy/gateway system. Each node is shipped as a binary and should be installable on new servers with minimal config. Nodes need shared state like sessions, user creds, quotas, and proxy pool data.\n\na. My current proposal is: each node talks only to a central internal API using a node key. That API handles all reads/writes to Redis/DB. This gives me tighter control over node onboarding, revocation, and limits blast radius if a node is ever compromised. It also avoids putting datastore credentials on every node.\n\nb. An alternative design (suggested by an LLM during architecture exploration) is letting every node connect directly to Redis for hot-path data (sessions, quotas, counters) and use it as the shared state layer, skipping the API hop. -- i didn't like the idea too much but the LLM kept defending it every time so maybe i am missin something!?! \n\nI’m trying to decide which pattern is more appropriate in practice for systems like gateways/proxies/workers: direct datastore access from each node, or API-mediated access only.\n\nWould like feedback from people who’ve run distributed production systems.", "full_text": "Design choice question: should distributed gateway nodes access datastore directly or only through an internal API?\n\nContext:  \nI’m building a horizontally scaled proxy/gateway system. Each node is shipped as a binary and should be installable on new servers with minimal config. Nodes need shared state like sessions, user creds, quotas, and proxy pool data.\n\na. My current proposal is: each node talks only to a central internal API using a node key. That API handles all reads/writes to Redis/DB. This gives me tighter control over node onboarding, revocation, and limits blast radius if a node is ever compromised. It also avoids putting datastore credentials on every node.\n\nb. An alternative design (suggested by an LLM during architecture exploration) is letting every node connect directly to Redis for hot-path data (sessions, quotas, counters) and use it as the shared state layer, skipping the API hop. -- i didn't like the idea too much but the LLM kept defending it every time so maybe i am missin something!?! \n\nI’m trying to decide which pattern is more appropriate in practice for systems like gateways/proxies/workers: direct datastore access from each node, or API-mediated access only.\n\nWould like feedback from people who’ve run distributed production systems.", "author": "GoldenSword-", "permalink": "/r/dataengineering/comments/1r0ht6o/design_choice_question_should_distributed_gateway/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0ht6o/design_choice_question_should_distributed_gateway/", "score": 3, "num_comments": 1, "upvote_ratio": 0.81, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0fky1", "created_utc": 1770668995.0, "title": "Visualizing full warehouse schemas is useless, so I built an ERD tool that only renders the tables you're working on", "selftext": "Dev here, (Full disclosure: I built this)  \n  \nFirst off I couldn't find any ERD that would give you:\n\n* A built-in MySQL editor\n* Diagrams rendered on the fly\n* Visualization of only the tables I need to see at that moment\n\nThe majority of websites came up with their own proprietary syntax or didn't have an editor at all. The ERD I built automatically syncs the cursor with the diagram showing the relationships you highlight in code.\n\nThe whole point of the project: warehouse-style schemas if visualized are useless. Visualizing FK relationships of tables I need to see on the fly is very helpful.\n\nFeedback is much appreciated!\n\nThe app: [sqlestev.com/dashboard](http://sqlestev.com/dashboard)", "full_text": "Visualizing full warehouse schemas is useless, so I built an ERD tool that only renders the tables you're working on\n\nDev here, (Full disclosure: I built this)  \n  \nFirst off I couldn't find any ERD that would give you:\n\n* A built-in MySQL editor\n* Diagrams rendered on the fly\n* Visualization of only the tables I need to see at that moment\n\nThe majority of websites came up with their own proprietary syntax or didn't have an editor at all. The ERD I built automatically syncs the cursor with the diagram showing the relationships you highlight in code.\n\nThe whole point of the project: warehouse-style schemas if visualized are useless. Visualizing FK relationships of tables I need to see on the fly is very helpful.\n\nFeedback is much appreciated!\n\nThe app: [sqlestev.com/dashboard](http://sqlestev.com/dashboard)", "author": "Spiritual_Ganache453", "permalink": "/r/dataengineering/comments/1r0fky1/visualizing_full_warehouse_schemas_is_useless_so/", "url": "https://i.redd.it/706ayxj8yiig1.gif", "score": 117, "num_comments": 3, "upvote_ratio": 1.0, "over_18": false}
{"source": "reddit", "subreddit": "dataengineering", "collected_at": "2026-02-12T14:40:52.604290+00:00", "post_id": "1r0ff3b", "created_utc": 1770668648.0, "title": "[AMA] We’re dbt Labs, ask us anything!", "selftext": "Hi r/dataengineering — though some might say analytics and data engineering are not the same thing, there’s still a great deal of dbt discussion happening here. So much so that the superb mods here have graciously offered to let us host an AMA happening this **Wednesday, February 11 at 12pm ET.**\n\nWe’ll be here to answer your questions about anything (though preferably about dbt things)\n\n**As an introduction, we are:**\n\n* Anders u/andersdellosnubes (DX Advocate) ([obligatory proof](https://i.imgur.com/4WzEcKM.jpeg))\n* Jason u/dbt-Jason (Director: DX, Community &amp; AI)\n* Jeremy Cohen u/jtcohen6 (PM) ([proof](https://imgur.com/rGUclDq))\n* Grace Goheen u/dbt-grace (PM) ([extra extra proof](https://i.imgur.com/pGMhBlk.gif))\n* Sara u/schemas_sgski (Product Marketing)\n* Quigley u/dbt-quigley (dbt Core engineer) ([proof](https://imgur.com/a7e89c8b-ee7d-42d3-a249-0fa68fe8d928))\n* Zeeshan u/dbt-zeeshan (Core engineering manager) ([proof](https://i.imgur.com/EkgG2dC.jpeg))\n* Tristan Handy u/jthandy (founder/CEO)\n\n**Here’s some questions that you might have for us:**\n\n* [what’s new](https://github.com/dbt-labs/dbt-core/releases/tag/v1.11.0) in dbt Core 1.11? what’s [coming next](https://github.com/dbt-labs/dbt-core/blob/main/docs/roadmap/2025-12-magic-to-do.md)?\n* what’s the latest in AI and agentic analytics ([MCP server](https://docs.getdbt.com/blog/introducing-dbt-mcp-server), [ADE bench](https://www.getdbt.com/blog/ade-bench-dbt-data-benchmarking), [dbt agent skills](https://docs.getdbt.com/blog/dbt-agent-skills))\n* what’s [the latest](https://github.com/dbt-labs/dbt-fusion/blob/main/CHANGELOG.md) with Fusion? is general availability coming anytime soon?\n* who is to blame to `nodes_to_a_grecian_urn` corny classical reference in our [docs site](https://docs.getdbt.com/reference/node-selection/yaml-selectors)?\n* is it true that we all get goosebumps anytime anytime someone types dbt with a capital d?\n\nDrop questions in the thread now or join us live on Wednesday!\n\nP.S. there’s a dbt Core 1.11 live virtual event next Thursday February 19. It will have live demos, cover roadmap, and prizes! [Save your seat here](https://www.getdbt.com/resources/webinars/dbt-core-1-11-live-release-updates-roadmap/?utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=q1-2027_dbt-core-live_aw&amp;utm_content=themed-webinar____&amp;utm_term=all_all__).\n\nedit: Hey we're live now and jumping in!\n\n&gt;thanks everyone for your questions! we all had a great time. we'll check back in on the thread throughout the day for any follow ups!\n&gt;\n&gt;If you want to know more about dbt Core 1.11, next week there's a live event next week!\n&gt;\n&gt;[reserve your spot here](https://www.getdbt.com/resources/webinars/dbt-core-1-11-live-release-updates-roadmap/?utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=q1-2027_dbt-core-live_aw&amp;utm_content=themed-webinar____&amp;utm_term=all_all__)", "full_text": "[AMA] We’re dbt Labs, ask us anything!\n\nHi r/dataengineering — though some might say analytics and data engineering are not the same thing, there’s still a great deal of dbt discussion happening here. So much so that the superb mods here have graciously offered to let us host an AMA happening this **Wednesday, February 11 at 12pm ET.**\n\nWe’ll be here to answer your questions about anything (though preferably about dbt things)\n\n**As an introduction, we are:**\n\n* Anders u/andersdellosnubes (DX Advocate) ([obligatory proof](https://i.imgur.com/4WzEcKM.jpeg))\n* Jason u/dbt-Jason (Director: DX, Community &amp; AI)\n* Jeremy Cohen u/jtcohen6 (PM) ([proof](https://imgur.com/rGUclDq))\n* Grace Goheen u/dbt-grace (PM) ([extra extra proof](https://i.imgur.com/pGMhBlk.gif))\n* Sara u/schemas_sgski (Product Marketing)\n* Quigley u/dbt-quigley (dbt Core engineer) ([proof](https://imgur.com/a7e89c8b-ee7d-42d3-a249-0fa68fe8d928))\n* Zeeshan u/dbt-zeeshan (Core engineering manager) ([proof](https://i.imgur.com/EkgG2dC.jpeg))\n* Tristan Handy u/jthandy (founder/CEO)\n\n**Here’s some questions that you might have for us:**\n\n* [what’s new](https://github.com/dbt-labs/dbt-core/releases/tag/v1.11.0) in dbt Core 1.11? what’s [coming next](https://github.com/dbt-labs/dbt-core/blob/main/docs/roadmap/2025-12-magic-to-do.md)?\n* what’s the latest in AI and agentic analytics ([MCP server](https://docs.getdbt.com/blog/introducing-dbt-mcp-server), [ADE bench](https://www.getdbt.com/blog/ade-bench-dbt-data-benchmarking), [dbt agent skills](https://docs.getdbt.com/blog/dbt-agent-skills))\n* what’s [the latest](https://github.com/dbt-labs/dbt-fusion/blob/main/CHANGELOG.md) with Fusion? is general availability coming anytime soon?\n* who is to blame to `nodes_to_a_grecian_urn` corny classical reference in our [docs site](https://docs.getdbt.com/reference/node-selection/yaml-selectors)?\n* is it true that we all get goosebumps anytime anytime someone types dbt with a capital d?\n\nDrop questions in the thread now or join us live on Wednesday!\n\nP.S. there’s a dbt Core 1.11 live virtual event next Thursday February 19. It will have live demos, cover roadmap, and prizes! [Save your seat here](https://www.getdbt.com/resources/webinars/dbt-core-1-11-live-release-updates-roadmap/?utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=q1-2027_dbt-core-live_aw&amp;utm_content=themed-webinar____&amp;utm_term=all_all__).\n\nedit: Hey we're live now and jumping in!\n\n&gt;thanks everyone for your questions! we all had a great time. we'll check back in on the thread throughout the day for any follow ups!\n&gt;\n&gt;If you want to know more about dbt Core 1.11, next week there's a live event next week!\n&gt;\n&gt;[reserve your spot here](https://www.getdbt.com/resources/webinars/dbt-core-1-11-live-release-updates-roadmap/?utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=q1-2027_dbt-core-live_aw&amp;utm_content=themed-webinar____&amp;utm_term=all_all__)", "author": "andersdellosnubes", "permalink": "/r/dataengineering/comments/1r0ff3b/ama_were_dbt_labs_ask_us_anything/", "url": "https://www.reddit.com/r/dataengineering/comments/1r0ff3b/ama_were_dbt_labs_ask_us_anything/", "score": 132, "num_comments": 118, "upvote_ratio": 0.91, "over_18": false}
